[
  {
    "objectID": "GPR.html",
    "href": "GPR.html",
    "title": "Gaussian Process Regression",
    "section": "",
    "text": "To illustrate Gaussian Process regression, we use an exponentiated quadratic kernel with input warping.\nTo reproduce the results, it is necessary to prepare the data set, plot base, and training and test data sets, as outlined in the “Data Preparation” section.\nNote that we ran these analyses on a Computation Server for more computational power.",
    "crumbs": [
      "Gaussian Process Regression"
    ]
  },
  {
    "objectID": "GPR.html#preparation",
    "href": "GPR.html#preparation",
    "title": "Gaussian Process Regression",
    "section": "Preparation",
    "text": "Preparation\n\nLoading Required Packages and Data\nLoad the necessary packages, data sets, and other supporting files. Each element serves a specific purpose:\n\ntidyverse: For data manipulation and visualisation.\nlgpr: To fit the Gaussian Process regression.\nrstan: Required by lgpr.\nmvtnorm: For experimenting with prior distributions.\ncaret: To compute model performance indices.\nplot_base: A pre-configured ggplot object for visualisation.\nTraining and Test Data sets: Required for cross-validation.\n\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(lgpr)\nlibrary(rstan)\nlibrary(caret)\nlibrary(mvtnorm)\n\n# Load the data set\nload(\"data/wido.rdata\")\n\n# Load the pre-configured plot base\nplot_base &lt;- readRDS(\"objects/plot_base.rds\")\n\n# Load training and test datasets for cross-validation\ntraining_datasets &lt;- readRDS(\"objects/training_datasets.rds\")\ntest_datasets &lt;- readRDS(\"objects/test_datasets.rds\")",
    "crumbs": [
      "Gaussian Process Regression"
    ]
  },
  {
    "objectID": "GPR.html#prior-specification",
    "href": "GPR.html#prior-specification",
    "title": "Gaussian Process Regression",
    "section": "Prior specification",
    "text": "Prior specification\nThe lgpr package adopts a fully Bayesian approach, necessitating prior definitions for parameters. A prior is a probability distribution that reflects initial assumptions about a parameter. As the parameters in our kernel cannot be negative, we use log-normal distributions to define our priors, as these are non-negative distributions.\n\nMagnitude Parameter\nIn the lgpr package, the response variable is automatically standardised to have a mean of 0 and an SD of 1 to speed up estimation. Consequently, we specify the magnitude prior to reflect expected changes on this standardised scale. To do so, we first examine the range of life satisfaction values on the standardised scale and the anticipated changes within this range.\n\n# Standardise life satisfaction values\nwido$lifesatisfaction_s &lt;- scale(wido$lifesatisfaction)\n\n# Explore range, mean and SD of ORIGINAL life satisfaction values\nrange(wido$lifesatisfaction)\n\n[1] 1 7\n\nmean(wido$lifesatisfaction)\n\n[1] 4.965978\n\nsd(wido$lifesatisfaction) \n\n[1] 1.108347\n\n# Explore range, mean and SD of STANDARDISED life satisfaction values\nwido$lifesatisfaction_s &lt;- scale(wido$lifesatisfaction)\nrange(wido$lifesatisfaction_s)\n\n[1] -3.578282  1.835186\n\nmean(wido$lifesatisfaction_s)\n\n[1] -4.824737e-15\n\nsd(wido$lifesatisfaction_s) \n\n[1] 1\n\n# Plot original against standardised values\nplot(wido$lifesatisfaction, wido$lifesatisfaction_s)\n\n\n\n\n\n\n\n# Plot the average smoothed change in original values on original scale (1 to 7)\nggplot() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = \"black\")) +\n  labs(x = \"Months from Widowhood\", y = \"Life Satisfaction Original\") +\n  scale_y_continuous(breaks = seq(1, 7, by = 1), limits = c(1, 7)) +\n  scale_x_continuous(breaks = seq(-180, 180, by = 30), limits = c(-180, 180)) +\n  theme(legend.position = \"none\") +\n  theme(aspect.ratio=0.9) +\ngeom_smooth(data = wido, aes(x=mnths, y=lifesatisfaction)) +\n  ggtitle(\"Average Life Satisfaction Change On Original Scale\") +\n  theme(plot.title = element_text(size = 13, face = \"bold\")) \n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n# Plot the average smoothed change in standardised values on standardised scale (-3 to 2)\nggplot() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = \"black\")) +\n  labs(x = \"Months from Widowhood\", y = \"Life Satisfaction Standardised\") +\n  scale_y_continuous(breaks = seq(-3, 2, by = 1), limits = c(-3.58, 1.84)) +\n  scale_x_continuous(breaks = seq(-180, 180, by = 30), limits = c(-180, 180)) +\n  theme(legend.position = \"none\") +\n  theme(aspect.ratio=0.9) +\n  geom_smooth(data = wido, aes(x=mnths, y=lifesatisfaction_s)) +\n  ggtitle(\"Average Life Satisfaction Change On Standardised Scale\") +\n  theme(plot.title = element_text(size = 13, face = \"bold\")) \n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nFrom this exploration, we observed that on the original scale, changes are unlikely to exceed 4 points, corresponding to a deviation from the mean of approximately 5 to 1 (the lower bound). On the standardised scale, however, the changes are expected to remain within 3 points, representing a deviation from the mean of 0 to -3 (lower bound).\nOur goal is to set a prior that is uninformative yet represents realistic parameter ranges. This entails allowing for changes of approximately 3 points or less. To achieve this, we employ a log-normal distribution with mean = 0 and sd = 1. By modifying the mean and sd in the code below, alternative prior distributions can be explored.\n\n# Define mean and SD of the log-normal distribution\nmean_priordistribution_magnitude &lt;- 0\nsd_priordistribution_magnitude &lt;- 1\n\n# Create a sequence of x values (range of values to plot)\nx &lt;- seq(0, 4, length.out = 1000)\n# Calculate the log-normal density for each x value\ny &lt;- dlnorm(x, meanlog = mean_priordistribution_magnitude, sdlog = sd_priordistribution_magnitude)\n\n# Plot the log-normal distribution to see what magnitude values are expected based on this prior\nplot(x, y, type = \"l\", col = \"blue\", lwd = 2, \n     main = \"Log-normal Distribution (mean = 0, sd = 1)\", \n     xlab = \"Magnitude\", ylab = \"Probability Density\")\n\n\n\n\n\n\n\n\n\n\nWarping Parameter\nFor the warping parameter, we expect most of the change to occur within 3 years before and after widowhood. Yet, again, we want to specify an uninformative prior and therefore also allow for wider or more narrow windows of change. We again use a log-normal distribution. To see how the warping function, and thereby the window of change, changes based on different priors for the warping parameter, the values of the mean and sd can be changed in the code below. The red lines in the plot indicate the time window in which this warping function allows changes to occur.\n\n# Define mean and SD of the log-normal distribution\nmean_priordistribution_warping &lt;- -0.7\nsd_priordistribution_warping &lt;- 0.9\n\n# Take draws from this distribution\nwrp_draws &lt;- stats::rlnorm(300, meanlog = mean_priordistribution_warping, sdlog = sd_priordistribution_warping)\n\n# Plot the resulting input warping function draws\nlgpr:::plot_inputwarp(wrp_draws, seq(-100, 100, by = 1), alpha = 0.1) +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank()) +\n  labs(x = \"Time\", y = \"Can changes occur?\", title = \"Prior draws for input warping function\") +\n  scale_y_continuous(breaks = c(-1, -0.5, 0, 0.5, 1), labels = c(\"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"))\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nThe prior for the warping function depicted above, a log-normal distribution with mean = -0.7 and sd = 0.9, indicates that we expect most changes within [-36, 36] months surrounding widowhood, but also allows for changes in more narrow or broader time windows (e.g., [-100, 100] months surrounding widowhood).\n\n\nLengthscale Parameter\nAs the effect of the lengthscale interacts with that of the warping parameter, its prior is best identified by considering all priors specified so far in concert. We again use a log-normal distribution. In the code below, the values of the mean and sd can be modified to inspect how different prior distributions for the lengthscale result in different possible trajectories. Note that the code below can also be used to experiment with different priors for the other parameters, by keeping the lengthscale prior distribution fixed.\n\n# For reproducibility\nset.seed(123)\n\n# Define mean and SD of the log-normal distribution\nmean_priordistribution_lengthscale &lt;- 0\nsd_priordistribution_lengthscale &lt;- 1\n\n# Define warping function\nwarpfun &lt;- function(time,warpingparameter){\n  2 / (1 + exp( - warpingparameter * time)) - 1\n}\n\n# Define kernel with warping function\nkernelGP &lt;- function(X1, X2, lengthscale, alpha, warpingparameter, fun=function(x,w){x}){\n  \n  X1mat &lt;- matrix(rep(fun(X1,warpingparameter),length(X2)),nrow=length(X1))\n  X2mat &lt;- matrix(rep(fun(X2,warpingparameter),length(X1)),nrow=length(X1),byrow=TRUE)\n  \n  alpha**2 * exp(- .5 * (X1mat - X2mat) * (X1mat - X2mat) / lengthscale**2 )\n  \n}\n\nn &lt;- 100 \nX &lt;- seq(-100, 100, length=n) \nmu = rep(0,length(X))\n\n# Set up a 4x4 plotting panel\npar(mfrow = c(4, 4), mar = c(2, 2, 2, 2))\n\n# Loop to generate 16 plots\nfor (i in 1:16) {\n\n# Lengthscale parameter\nlengthscale1 &lt;- dlnorm(1, meanlog = mean_priordistribution_lengthscale, sdlog = sd_priordistribution_lengthscale)\n\n# Magnitude parameter (potentially modify prior distribution here!)\nalpha1 &lt;- dlnorm(1, meanlog = 0, sdlog = 1)\n\n# Warping parameter (potentially modify prior distribution here!)\nwarpingparameter &lt;- dlnorm(1, meanlog=-0.7,sdlog=0.9)\ncovm = kernelGP(X, X, lengthscale = lengthscale1, alpha = alpha1, warpingparameter = warpingparameter, fun = warpfun)\n\n# Intercept\nbeta0 &lt;- 0\n\n  # Draw a line from the GP\n  draw_f &lt;- mvtnorm::rmvnorm(1, mean = mu, sigma = covm)\n  \n  # Plot the line\n  plot(X, beta0 + draw_f, type = \"l\", col = \"grey\", lwd = 2,\n       ylim = c(beta0 - 2 * alpha1, beta0 + 2 * alpha1),\n       ylab = \"y\", xlab = \"x\", main = paste(\"Plot\", i))\n}\n\n\n\n\n\n\n\n\nA log-normal prior distribution with mean = 0 and sd = 1for the lengthscale, together with the magnitude and warping parameter priors identified earlier, can result in pretty different, but within this context realistic trajectories. These priors thus are relatively uninformative, while ensuring realistic bounds for the parameters. Therefore, this prior is specified. Note that in lgpr the magnitude parameter is called alpha.\n\nprior &lt;- list(\n  alpha = log_normal(0, 1), \n  ell = log_normal(0, 1),\n  wrp = log_normal(-0.7, 0.9)\n)",
    "crumbs": [
      "Gaussian Process Regression"
    ]
  },
  {
    "objectID": "GPR.html#analysis",
    "href": "GPR.html#analysis",
    "title": "Gaussian Process Regression",
    "section": "Analysis",
    "text": "Analysis\n\nFitting the Model\nWe first attempt to estimate person-specific deviations for all parameters (magnitude, lengthscale, and warping) in addition to the overall (“fixed”) effect of time. This approach is comparable to including random intercepts and slopes in a linear model.\n\n# For reproducibility\nset.seed(123)\n\n# Fit model\ngp &lt;- lgp(lifesatisfaction ~ gp_ns(mnths) + gp_ns(mnths) * zs(id), data = wido, prior = prior)\n\n\n\n\n\n\n\n\n\n\nA quick glance at this model indicates that it acts a bit strange: it estimates more than 2 points change in life satisfaction on a scale of 1 to 7, which is twice as large as in all other models considered. We suspect the model is somewhat too complicated to be estimated reliably with this number of observations per individual.\nWe therefore fit a simpler model estimating stable person-specific deviations from the average trajectory, analogous to random intercepts.\n\ngp &lt;- lgp(lifesatisfaction ~ gp_ns(mnths) + zs(id), data = wido, prior = prior)\n\n\n\n\n\n\n\n\n\n\nThis adjustment results in change estimates that are more in line with the results of the other models. The function below prints the posterior distributions of the parameters. The first alpha is the magnitude of the shared time effect, the second alpha is the person-specific time effect.\n\n\nAn object of class lgpfit. See ?lgpfit for more info.\nInference for Stan model: lgp.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n         mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat\nalpha[1] 0.43    0.01 0.23 0.20 0.29 0.37 0.50  1.04  1404 1.00\nalpha[2] 0.73    0.00 0.04 0.66 0.70 0.73 0.76  0.81  2619 1.00\nell[1]   0.37    0.01 0.20 0.18 0.26 0.31 0.39  0.98   485 1.01\nwrp[1]   0.05    0.00 0.02 0.03 0.04 0.05 0.06  0.12   516 1.01\nsigma[1] 0.64    0.00 0.01 0.62 0.63 0.64 0.64  0.66  2597 1.00\n\nSamples were drawn using NUTS(diag_e) at Tue Dec 17 01:12:35 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe lgpr package standardises the response variable to a mean of zero and standard deviation of one. To interpret magnitude estimates on the original scale, they are multiplied by the original variance.\nThe magnitude of the shared time-effect is:\n\nround(0.43*var(wido$lifesatisfaction), 2)\n\n[1] 0.53\n\n\nThe magnitude of the person-specific time-effect is:\n\nround(0.73*var(wido$lifesatisfaction), 2)\n\n[1] 0.9",
    "crumbs": [
      "Gaussian Process Regression"
    ]
  },
  {
    "objectID": "GPR.html#visualisation",
    "href": "GPR.html#visualisation",
    "title": "Gaussian Process Regression",
    "section": "Visualisation",
    "text": "Visualisation\n\nStandard Deviation\nTo visualise the results, we depict the shared and person-specific trajectories as predicted by the posterior means of the parameters. The posterior mean is the mean of the posterior distribution of the parameter. It is the expected value of the parameter, given the data and prior. To show the uncertainty of the estimated shared trajectory, we plot the interval between two standard deviations from the posterior mean. Within this interval we expect the true trajectory to lie with approximately 95% probability.\n\n\nPredicting Average and Individual Trajectories\nPredict both the shared (fixed effects) and person-specific (random effects) trajectories of life satisfaction. We create an extra function below to wrangle the predictions in the format we need for our plot.\n\n# Define time sequence\nt &lt;- seq(-178, 179, by = 1)\n\n# Generate prediction dataset\nx_pred &lt;- new_x(wido, t, group_by = NA, x = \"mnths\")\n\n\n# Create function to wrangle the predictions to the correct format\nprocess_predictions &lt;- function(predicted) {\n  # Extract and reshape predictions (mean and standard deviation)\n  x &lt;- as.data.frame(predicted@x)\n  y &lt;- as.data.frame(predicted@y_mean) %&gt;%\n    pivot_longer(cols = everything()) %&gt;%\n    dplyr::select(-name) %&gt;%\n    rename(y = value)\n  \n  std &lt;- as.data.frame(predicted@y_std) %&gt;%\n    pivot_longer(cols = everything()) %&gt;%\n    dplyr::select(-name) %&gt;%\n    rename(std = value)\n  \n  # Combine and calculate bounds\n  data &lt;- cbind(x, y, std) %&gt;%\n    mutate(lower = y - std * 1.96, \n           upper = pmin(y + std * 1.96, 7))  # Cap upper bound at 7\n  return(data)\n}\n\n# Apply function for predictions for shared effect (f = fixed effect) and person-specific effect (r = random effects)\ngp_pred_f &lt;- process_predictions(pred(gp, x_pred, reduce = mean, verbose = FALSE))\ngp_pred_r &lt;- process_predictions(pred(gp, reduce = mean, verbose = FALSE))\n\n\n\nSelecting a Random Sample for Plotting\nFor better visualisation, select a random sample of individuals to display their individual trajectories.\n\n# For reproducibility\nset.seed(123)\n\n# Randomly sample 50 participants\nrsample_ids &lt;- sample(unique(wido$id), 50)\n\n# Filter the data to include only the randomly selected participants\ngp_pred_r_rsample &lt;- gp_pred_r %&gt;%\n  filter(id %in% rsample_ids)\n\n\n\nCreating the Plot\nCombine all elements to create the plot, which includes individual trajectories, the population trajectory, and its standard deviation.\n\nplot_base + \n  geom_smooth(data = gp_pred_r_rsample, aes(mnths, y, group = id), se = F, span = 0.90, color = \"grey70\", linewidth = 0.4) +\n  geom_ribbon(data = gp_pred_f, aes(ymin = lower, ymax = upper, x = mnths),\n              alpha = 0.2, fill = \"firebrick4\") +\n  geom_line(data = gp_pred_f, aes(x = mnths, y = y),  color = \"firebrick4\", linewidth = 1) +\n  ggtitle(\"Gaussian Process Regression\") +\n  theme(plot.title = element_text(size = 13, face = \"bold\"))",
    "crumbs": [
      "Gaussian Process Regression"
    ]
  },
  {
    "objectID": "GPR.html#model-performance",
    "href": "GPR.html#model-performance",
    "title": "Gaussian Process Regression",
    "section": "Model Performance",
    "text": "Model Performance\n\nEvaluating the Model\nAssess the model’s performance using the Bayesian Information Criterion (BIC), R-squared (R²), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).\n\n# Because we want to compare our predicted average trajectory, with the actual average trajectory, we need to merge the predicted dataframe with the original data\ngp_pred_f &lt;- gp_pred_f %&gt;%\n  dplyr::select(mnths, y)\ngp_pred_f &lt;- merge(wido, gp_pred_f, key = \"mnths\")\n\n# Now we can calculate R², MAE, and RMSE for the fixed effects predictions\ndata.frame(\n  R2_FE = round(R2(gp_pred_f$y, gp_pred_f$m_lifesat_per_mnth), 2),\n  MAE_FE = round(MAE(gp_pred_f$y, gp_pred_f$m_lifesat_per_mnth), 2),\n  RMSE_FE = round(RMSE(gp_pred_f$y, gp_pred_f$m_lifesat_per_mnth), 2)\n)\n\n  R2_FE MAE_FE RMSE_FE\n1  0.32   0.28    0.38\n\n# Calculate R², MAE, and RMSE for the random effects predictions\ndata.frame(\n  R2_RE = round(R2(gp_pred_r$y, wido$lifesatisfaction), 2),\n  MAE_RE = round(MAE(gp_pred_r$y, wido$lifesatisfaction), 2),\n  RMSE_RE = round(RMSE(gp_pred_r$y, wido$lifesatisfaction), 2)\n)\n\n  R2_RE MAE_RE RMSE_RE\n1  0.63   0.51    0.67\n\n\n\n\nCross-Validation\nTo assess the replicability of the model, perform cross-validation using the training and test datasets. For each training dataset, fit the model and compute performance metrics for the associated test dataset R², MAE, and RMSE.\n\n# Initialize vectors to store the performance metrics\nR2_values_gp &lt;- c()\nRMSE_values_gp &lt;- c()\nMAE_values_gp &lt;- c()\n\n# Loop over the datasets\nfor (i in 1:length(training_datasets)) {\n  # Get the current training and test dataset\n  training_data &lt;- training_datasets[[i]]\n  test_data &lt;- test_datasets[[i]]\n  \n  # Fit the model\n  gp &lt;- lgp(lifesatisfaction ~ gp_ns(mnths) + zs(id),\n            data     = training_data,\n            iter     = 500,\n            chains   = 4,\n            refresh  = 10, \n            prior = prior)\n  \n  # Compute average test trajectory\n  test_data &lt;- test_data %&gt;%\n    group_by(mnths) %&gt;%\n    mutate(m_lifesat_per_mnth = mean(lifesatisfaction, na.rm = TRUE))\n  \n  # Predict fixed effects\n  x_pred &lt;- new_x(test_data, seq((min(test_data$mnths)), (max(test_data$mnths)), by = 1), group_by = NA, x = \"mnths\")\n  pred_gp_f &lt;- pred(gp_model, x_pred, reduce = mean, verbose = FALSE)\n  \n  gp_pred_x_f &lt;- as.data.frame(pred_gp_f@x)\n  gp_pred_y_f &lt;- as.data.frame(pred_gp_f@y_mean)\n  gp_pred_y_f &lt;- pivot_longer(gp_pred_y_f, cols = everything())\n  gp_pred_y_f &lt;- dplyr::select(gp_pred_y_f, -name)\n  gp_pred_y_f &lt;- rename(gp_pred_y_f, gp_pred_ls_f = value)\n  gp_pred_all_f &lt;- cbind(gp_pred_x_f, gp_pred_y_f)\n  \n  pred_gp_f &lt;- gp_pred_all_f %&gt;%\n    dplyr::select(mnths, gp_pred_ls_f)\n  \n  pred_gp_f &lt;- merge(test_data, pred_gp_f, key = \"mnths\")\n  \n  # Compute performance metrics\n  R2_value &lt;- R2(pred_gp_f$gp_pred_ls_f, pred_gp_f$m_lifesat_per_mnth)\n  RMSE_value &lt;- RMSE(pred_gp_f$gp_pred_ls_f, pred_gp_f$m_lifesat_per_mnth)\n  MAE_value &lt;- MAE(pred_gp_f$gp_pred_ls_f, pred_gp_f$m_lifesat_per_mnth)\n  \n  # Store the metrics\n  R2_values_gp &lt;- c(R2_values_gp, R2_value)\n  RMSE_values_gp &lt;- c(RMSE_values_gp, RMSE_value)\n  MAE_values_gp &lt;- c(MAE_values_gp, MAE_value)\n}\n\n# Compute average performance metrics (mean)\naverage_R2_gp &lt;- mean(R2_values_gp)\naverage_RMSE_gp &lt;- mean(RMSE_values_gp)\naverage_MAE_gp &lt;- mean(MAE_values_gp)\n\n# Compute standard deviation of performance metrics (SD)\nsd_R2_gp &lt;- sd(R2_values_gp)\nsd_RMSE_gp &lt;- sd(RMSE_values_gp)\nsd_MAE_gp &lt;- sd(MAE_values_gp)\n\n# Combine the mean and standard deviation into one data.frame\ncombined_metrics_gp &lt;- data.frame(\n  Metric = c(\"R²\", \"MAE\", \"RMSE\"),\n  Mean = round(c(average_R2_gp, average_MAE_gp, average_RMSE_gp), 2),\n  SD = round(c(sd_R2_gp, sd_MAE_gp, sd_RMSE_gp), 2)\n)\n\n# Print the combined metrics\nprint(combined_metrics_gp)\n\n\n\n  Metric Mean   SD\n1     R² 0.12 0.04\n2    MAE 0.72 0.16\n3   RMSE 0.90 0.20",
    "crumbs": [
      "Gaussian Process Regression"
    ]
  },
  {
    "objectID": "PW.html",
    "href": "PW.html",
    "title": "Piecewise Regression",
    "section": "",
    "text": "To illustrate piecewise regression, we fit a two-piece linear-linear model.\nTo reproduce the results, it is necessary to prepare the data set, plot base, and training and test data sets, as outlined in the “Data Preparation” section.",
    "crumbs": [
      "Piecewise Regression"
    ]
  },
  {
    "objectID": "PW.html#preparation",
    "href": "PW.html#preparation",
    "title": "Piecewise Regression",
    "section": "Preparation",
    "text": "Preparation\n\nLoading Required Packages and Data\nLoad the necessary packages, data sets, and other supporting files. Each element serves a specific purpose:\n\ntidyverse: For data manipulation and visualisation.\nlme4 and lmerTest: To fit and analyse mixed-effects models.\ncaret: To compute model performance indices.\nplot_base: A pre-configured ggplot object for visualisation.\nTraining and Test Data sets: Required for cross-validation.\n\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(caret)\n\n# Load the data set\nload(\"data/wido.rdata\")\n\n# Load the pre-configured plot base\nplot_base &lt;- readRDS(\"objects/plot_base.rds\")\n\n# Load training and test datasets for cross-validation\ntraining_datasets &lt;- readRDS(\"objects/training_datasets.rds\")\ntest_datasets &lt;- readRDS(\"objects/test_datasets.rds\")\n\n\n\nCreate time variables\nCreate time variables for the parameters of the segments:\n\npostD is a dummy variable with 0 for all measurements before the transition and 1 for all measurements after. This quantifies the shift in life satisfaction level post-transition.\npreLin has negative values indicating the time before the transition, and is 0 after the transition. This captures the rate of change in life satisfaction pre-transition.\npostLin, is 0 before the transition and has positive values afterward indicating the time after the transition. This captures the rate of change in life satisfaction post-transition.\n\nThe intercept captures the life satisfaction level before the transition.\n\n# Create time variables\nwido &lt;- wido %&gt;%\n  mutate(postD = if_else(mnths &lt;= 0, 0, 1),\n         preLin = if_else(mnths &lt;= 0, mnths, 0),\n         postLin = if_else(mnths &lt;= 0, 0, mnths))\n\nTo avoid multicollinearity because we use multiple (correlated) time variables in analysis, standardise the preLin and postLin variables.\n\n# Standardise preLin and postLin\nwido$preLin_s &lt;- scale(wido$preLin)\nwido$postLin_s &lt;- scale(wido$postLin)",
    "crumbs": [
      "Piecewise Regression"
    ]
  },
  {
    "objectID": "PW.html#analysis",
    "href": "PW.html#analysis",
    "title": "Piecewise Regression",
    "section": "Analysis",
    "text": "Analysis\n\nFitting the Model\nFit the piecewise model using the newly created (standardised) time variables. This model includes both fixed and random effects for the time terms to account for person-specific trajectories.\n\n# Fit the piecewise model\npw &lt;- lmer(\n  lifesatisfaction ~ postD + preLin_s + postLin_s + \n    (postD + preLin_s + postLin_s | id),\n  data = wido)\n\n# Display the summary of the model\nsummary(pw)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: lifesatisfaction ~ postD + preLin_s + postLin_s + (postD + preLin_s +  \n    postLin_s | id)\n   Data: wido\n\nREML criterion at convergence: 5239.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.9090 -0.4860  0.0641  0.5612  3.9181 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr             \n id       (Intercept) 0.76547  0.8749                    \n          postD       0.61449  0.7839   -0.44            \n          preLin_s    0.05851  0.2419    0.34 -0.09      \n          postLin_s   0.04506  0.2123    0.06 -0.19 -0.40\n Residual             0.35199  0.5933                    \nNumber of obs: 2322, groups:  id, 208\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   5.17060    0.06676 206.50957  77.451  &lt; 2e-16 ***\npostD        -0.42436    0.07103 211.09018  -5.974 9.70e-09 ***\npreLin_s     -0.16439    0.03013  90.22569  -5.456 4.22e-07 ***\npostLin_s     0.23386    0.03165  63.12876   7.389 4.15e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr) postD  prLn_s\npostD     -0.509              \npreLin_s   0.223 -0.301       \npostLin_s  0.247 -0.366 -0.061\n\n# Compute confidence intervals for the model parameters\nround(confint(pw), 2)\n\nComputing profile confidence intervals ...\n\n\n            2.5 % 97.5 %\n.sig01       0.78   0.98\n.sig02      -0.58  -0.27\n.sig03       0.08   0.56\n.sig04      -0.27   0.33\n.sig05       0.66   0.91\n.sig06      -0.39   0.26\n.sig07      -0.47   0.21\n.sig08       0.18   0.31\n.sig09      -1.00   0.24\n.sig10       0.13   0.29\n.sigma       0.57   0.61\n(Intercept)  5.04   5.30\npostD       -0.56  -0.28\npreLin_s    -0.23  -0.10\npostLin_s    0.17   0.30",
    "crumbs": [
      "Piecewise Regression"
    ]
  },
  {
    "objectID": "PW.html#visualisation",
    "href": "PW.html#visualisation",
    "title": "Piecewise Regression",
    "section": "Visualisation",
    "text": "Visualisation\n\nBootstrapping Confidence Intervals\nUse bootstrapping to estimate the confidence intervals for the predicted values of the model. This provides a robust measure of uncertainty.\n\n# For reproducibility\nset.seed(123)\n\n# Bootstrapping for confidence intervals of the predictions\nboot_results &lt;- bootMer(pw, FUN = function(x) predict(x, newdata = wido, re.form = NA),\n                        nsim = 1000)\n\n# Extract the 95% confidence intervals from the bootstrapped results\nci &lt;- apply(boot_results$t, 2, quantile, probs = c(0.025, 0.975))\n\n# Assign the lower and upper bounds to the data\nwido$lower_bound &lt;- ci[1, ]\nwido$upper_bound &lt;- ci[2, ]\n\n\n\nPredicting Average and Individual Trajectories\nPredict both the population-level (fixed effects) and individual-level (random effects) trajectories of life satisfaction.\n\n# Predict population-level trajectories based on fixed effects\nwido$lifesatisfaction_pw_f &lt;- predict(pw, newdata = wido, re.form = NA)\n\n# Predict individual-level trajectories based on random effects\nwido$lifesatisfaction_pw_r &lt;- predict(pw, newdata = wido, re.form = NULL)\n\n\n\nSelecting a Random Sample for Plotting\nFor better visualisation, select a random sample of individuals to display their individual trajectories.\n\n# For reproducibility\nset.seed(123)\n\n# Randomly sample 50 participants\nrsample_ids &lt;- sample(unique(wido$id), 50)\n\n# Filter the data to include only the randomly selected participants\nwido_rsample &lt;- wido %&gt;%\n  filter(id %in% rsample_ids)\n\n\n\nCreating the Plot\nCombine all elements to create the plot, which includes individual trajectories, the population trajectory, and the confidence interval of the population trajectory.\n\n# Create the plot using the pre-configured plot base\nplot_base +\n  geom_line(\n    data = wido_rsample,\n    aes(mnths, lifesatisfaction_pw_r, group = id),\n    color = \"grey70\",\n    linewidth = 0.4\n  ) +\n  geom_line(\n    data = wido,\n    aes(\n      x = mnths,\n      y = ifelse(mnths == 0, NA, lifesatisfaction_pw_f)\n      ),\n    color = \"firebrick4\",\n    linewidth = 1\n  ) +\n  geom_ribbon(\n    data = wido %&gt;% filter(mnths != 0),\n    aes(ymin = lower_bound, ymax = upper_bound, x = mnths),\n    alpha = 0.2,\n    fill = \"firebrick4\"\n  ) +\n  ggtitle(\"Piecewise Linear-Linear Model\") +\n  theme(plot.title = element_text(size = 13, face = \"bold\"))",
    "crumbs": [
      "Piecewise Regression"
    ]
  },
  {
    "objectID": "PW.html#model-performance",
    "href": "PW.html#model-performance",
    "title": "Piecewise Regression",
    "section": "Model Performance",
    "text": "Model Performance\n\nEvaluating the Model\nAssess the model’s performance using the Bayesian Information Criterion (BIC), R-squared (R²), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).\n\n# Compute BIC for the fitted model\nround(BIC(pw), 2)\n\n[1] 5356.08\n\n# Calculate R², MAE, and RMSE for the fixed effects predictions\ndata.frame(\n  R2_FE = round(R2(wido$lifesatisfaction_pw_f, wido$m_lifesat_per_mnth), 2),\n  MAE_FE = round(MAE(wido$lifesatisfaction_pw_f, wido$m_lifesat_per_mnth), 2),\n  RMSE_FE = round(RMSE(wido$lifesatisfaction_pw_f, wido$m_lifesat_per_mnth), 2)\n)\n\n  R2_FE MAE_FE RMSE_FE\n1  0.27   0.29    0.39\n\n# Calculate R², MAE, and RMSE for the random effects predictions\ndata.frame(\n  R2_RE = round(R2(wido$lifesatisfaction_pw_r, wido$lifesatisfaction), 2),\n  MAE_RE = round(MAE(wido$lifesatisfaction_pw_r, wido$lifesatisfaction), 2),\n  RMSE_RE = round(RMSE(wido$lifesatisfaction_pw_r, wido$lifesatisfaction), 2)\n)\n\n  R2_RE MAE_RE RMSE_RE\n1  0.77    0.4    0.53\n\n\n\n\nCross-Validation\nTo assess the replicability of the model, perform cross-validation using the training and test datasets. For each training dataset, fit the model and compute performance metrics for the associated test dataset R², MAE, and RMSE.\n\n# Initialise vectors to store performance metrics\nR2_values &lt;- c()\nMAE_values &lt;- c()\nRMSE_values &lt;- c()\n\n# Perform cross-validation\nfor (i in 1:length(training_datasets)) {\n  # Get the current training and test dataset\n  training_data &lt;- training_datasets[[i]]\n  test_data &lt;- test_datasets[[i]]\n  \n  # Create time variables\n  training_data &lt;- training_data %&gt;%\n  mutate(postD = if_else(mnths &lt;= 0, 0, 1),\n         preLin = if_else(mnths &lt;= 0, mnths, 0),\n         postLin = if_else(mnths &lt;= 0, 0, mnths))\n  \n  test_data &lt;- test_data %&gt;%\n  mutate(postD = if_else(mnths &lt;= 0, 0, 1),\n         preLin = if_else(mnths &lt;= 0, mnths, 0),\n         postLin = if_else(mnths &lt;= 0, 0, mnths))\n  \n  # Standardise preLin and postLin\n  training_data$preLin_s &lt;- scale(training_data$preLin)\n  training_data$postLin_s &lt;- scale(training_data$postLin)\n  \n  test_data$preLin_s &lt;- scale(test_data$preLin)\n  test_data$postLin_s &lt;- scale(test_data$postLin)\n  \n  # Fit the model\n  pw &lt;- lmer(\n    lifesatisfaction ~ postD + preLin_s + postLin_s + \n      (postD + preLin_s + postLin_s | id),\n    data = training_data)\n  \n  # Predict fixed effects\n  test_predictions &lt;- predict(pw, test_data, re.form = NA)\n  \n  # Compute average test trajectory\n  test_data &lt;- test_data %&gt;%\n    group_by(mnths) %&gt;%\n    mutate(m_lifesat_per_mnth = mean(lifesatisfaction, na.rm = TRUE))\n  \n  # Calculate performance metrics\n  R2_values &lt;- c(R2_values, R2(test_predictions, test_data$m_lifesat_per_mnth))\n  MAE_values &lt;- c(MAE_values, MAE(test_predictions, test_data$m_lifesat_per_mnth))\n  RMSE_values &lt;- c(RMSE_values, RMSE(test_predictions, test_data$m_lifesat_per_mnth))\n}\n\n# Compute average performance metrics (mean)\n  average_R2 &lt;- mean(R2_values)\n  average_MAE &lt;- mean(MAE_values)\n  average_RMSE &lt;- mean(RMSE_values)\n\n# Compute average performance metrics (SD)\n  sd_R2 &lt;- sd(R2_values)\n  sd_MAE &lt;- sd(MAE_values)\n  sd_RMSE &lt;- sd(RMSE_values)\n\n# Combine the mean and standard deviation into one data.frame\ncombined_metrics &lt;- data.frame(\n  Metric = c(\"R²\", \"MAE\", \"RMSE\"),\n  Mean = round(c(average_R2, average_MAE, average_RMSE), 2),\n  SD = round(c(sd_R2, sd_MAE, sd_RMSE), 2)\n)\n\n# Print the combined metrics\nprint(combined_metrics)\n\n  Metric Mean   SD\n1     R² 0.10 0.07\n2    MAE 0.58 0.08\n3   RMSE 0.76 0.13",
    "crumbs": [
      "Piecewise Regression"
    ]
  },
  {
    "objectID": "GAM.html",
    "href": "GAM.html",
    "title": "Generalised Additive Model",
    "section": "",
    "text": "To illustrate generalised additive models (GAM), we fit a model with a fixed smooth function of time, and random (person-specific) smooth functions of time.\nTo reproduce the results, it is necessary to prepare the data set, plot base, and training and test data sets, as outlined in the “Data Preparation” section.",
    "crumbs": [
      "Generalised Additive Model"
    ]
  },
  {
    "objectID": "GAM.html#preparation",
    "href": "GAM.html#preparation",
    "title": "Generalised Additive Model",
    "section": "Preparation",
    "text": "Preparation\n\nLoading Required Packages and Data\nLoad the necessary packages, data sets, and other supporting files. Each element serves a specific purpose:\n\ntidyverse: For data manipulation and visualisation.\nmgcv: To fit the generalised additive model.\ngratia: For obtaining the estimated slopes across the time points.\ntidygam: To compute the confidence intervals of the predictions.\ncaret: To compute model performance indices.\nplot_base: A pre-configured ggplot object for visualisation.\nTraining and Test Data sets: Required for cross-validation.\n\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(gratia)\nlibrary(tidygam)\nlibrary(caret)\n\n# Load the data set\nload(\"data/wido.rdata\")\n\n# Load the pre-configured plot base\nplot_base &lt;- readRDS(\"objects/plot_base.rds\")\n\n# Load training and test datasets for cross-validation\ntraining_datasets &lt;- readRDS(\"objects/training_datasets.rds\")\ntest_datasets &lt;- readRDS(\"objects/test_datasets.rds\")",
    "crumbs": [
      "Generalised Additive Model"
    ]
  },
  {
    "objectID": "GAM.html#analysis",
    "href": "GAM.html#analysis",
    "title": "Generalised Additive Model",
    "section": "Analysis",
    "text": "Analysis\n\nFitting the Model\nDetermine k. This defines the maximum possible complexity, or “wiggliness,” of the model. The exact choice of k is not critical; it should be large enough to represent the underlying pattern but small enough to maintain computational efficiency (Wood, 2017). We use a k of one fewer than the levels of the time variable, to allow for a wiggle with each new level (see also Doré & Bolger, 2018). Fit the GAM. We use a fixed smooth function of time, specifying k; and random (person-specific) smooth functions of time, using bs = \"fs\". The m = 1 argument sets a penalty for the smooth. Alternatively, to estimate only random intercepts specify s(id, bs = \"re\"). To estimate random slopes specify s(mnths, id, bs = \"re\").\n\n# Determine number of unique values of time to define k\nn_distinct(wido$mnths)\n\n[1] 334\n\n# Fit the generalised additive model \ngam &lt;- gam(lifesatisfaction ~ s(mnths, k = 333) + s(mnths, id, bs = \"fs\", m = 1), data = wido, method = \"REML\")\n\n# Display the summary of the model\nsummary(gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nlifesatisfaction ~ s(mnths, k = 333) + s(mnths, id, bs = \"fs\", \n    m = 1)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.98120    0.05945   83.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n               edf  Ref.df     F p-value    \ns(mnths)     17.13   21.26 8.587  &lt;2e-16 ***\ns(mnths,id) 504.75 1711.00 3.164  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.717   Deviance explained = 78.1%\n-REML = 2661.5  Scale est. = 0.3478    n = 2322\n\n\n\n\nInspect Slope Changes Over Time\nTo get an indication of when and how change in life satisfaction is taking place, examine how the slope (first derivative) changes across the range of time. Time periods where the confidence interval of the slope straddles zero indicate periods of stability. Positive slopes indicate increases, while negative slopes indicate declines. Compute the slopes and confidence intervals across the the values of time. To inspect them, the slopes-object created below can be printed, but here we only print the time intervals in which there is an increasing and a decreasing trend.\n\n# Compute the slopes (first derivatives) across the values of time\nslopes &lt;- derivatives(gam, n = 333, n_sim = 100, select = \"s(mnths)\")\n\n# Create a variable indicating whether there is an increasing or a decreasing trend, or stability.\nslopes &lt;- slopes %&gt;%\n    mutate(\n        trend = case_when(\n          .lower_ci &gt; 0 & .upper_ci &gt; 0 ~ \"increasing\",  \n          .lower_ci &lt; 0 & .upper_ci &lt; 0  ~ \"decreasing\",       \n          TRUE ~ \"stability\"))\n\n# Print range of time when there is a decreasing trend\nrange(slopes$mnths[slopes$trend == \"decreasing\"])\n\n[1] -18.855422   2.650602\n\n# Print range of time when there is an increasing trend\nrange(slopes$mnths[slopes$trend == \"increasing\"])\n\n[1] 10.17771 24.15663",
    "crumbs": [
      "Generalised Additive Model"
    ]
  },
  {
    "objectID": "GAM.html#visualisation",
    "href": "GAM.html#visualisation",
    "title": "Generalised Additive Model",
    "section": "Visualisation",
    "text": "Visualisation\n\nPredicting Average and Individual Trajectories\nPredict both the population-level (fixed effects) and individual-level (random effects) trajectories of life satisfaction.\n\n# Predict population-level trajectories based on fixed effects\nwido$lifesatisfaction_gam_f &lt;- predict(gam, newdata = wido, exclude=\"s(mnths,id)\")\n\n# Predict individual-level trajectories based on random effects\nwido$lifesatisfaction_gam_r &lt;- predict(gam, newdata = wido)\n\n\n\nCompute Confidence Intervals\nCompute the confidence intervals for the predicted values of the model using the predict_gam function from the tidygam package. ci_z = 1.96 estimates 95% confidence intervals.\n\n# Compute confidence intervals for 400 values of time (mnths)\nci &lt;- predict_gam(gam, exclude_terms = \"s(mnths,id)\", length_out = 400, ci_z = 1.96)\n\n# In the object with the confidence intervals, round down the values of time (mnths)\nci$mnths &lt;- round(ci$mnths, 0)\n\n# Select the relevant columns of the confidence intervals object\nci &lt;- ci %&gt;% select(mnths, lower_ci, upper_ci)\n\n# Merge the confidence intervals with the wido data frame based on the mnths column\nwido &lt;- wido %&gt;%\n  left_join(ci, by = \"mnths\")\n\n\n\nSelecting a Random Sample for Plotting\nFor better visualisation, select a random sample of individuals to display their individual trajectories.\n\n# For reproducibility\nset.seed(123)\n\n# Randomly sample 50 participants\nrsample_ids &lt;- sample(unique(wido$id), 50)\n\n# Filter the data to include only the randomly selected participants\nwido_rsample &lt;- wido %&gt;%\n  filter(id %in% rsample_ids)\n\n\n\nCreating the Plot\nCombine all elements to create the plot, which includes individual trajectories, the population trajectory, and the confidence interval of the population trajectory.\n\n# Create the plot using the pre-configured plot base\nplot_base +\n  geom_line(\n    data = wido_rsample,\n    aes(mnths, lifesatisfaction_gam_r, group = id),\n    color = \"grey70\",\n    linewidth = 0.4\n  ) +\n  geom_line(\n    data = wido,\n    aes(mnths, lifesatisfaction_gam_f),\n    color = \"firebrick4\",\n    linewidth = 1\n  ) +\n  geom_ribbon(\n    data = wido,\n    aes(\n      ymin = lower_ci,\n      ymax = upper_ci,\n      x = mnths\n    ),\n    alpha = 0.2,\n    fill = \"firebrick4\"\n  ) +\n  ggtitle(\"Generalised Additive Model\") +\n  theme(plot.title = element_text(size = 13, face = \"bold\"))",
    "crumbs": [
      "Generalised Additive Model"
    ]
  },
  {
    "objectID": "GAM.html#model-performance",
    "href": "GAM.html#model-performance",
    "title": "Generalised Additive Model",
    "section": "Model Performance",
    "text": "Model Performance\n\nEvaluating the Model\nAssess the model’s performance using the Bayesian Information Criterion (BIC), R-squared (R²), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).\n\n# Compute BIC for the fitted model\nround(BIC(gam), 2)\n\n[1] 7626.69\n\n# Calculate R², MAE, and RMSE for the fixed effects predictions\ndata.frame(\n  R2_FE = round(R2(wido$lifesatisfaction_gam_f, wido$m_lifesat_per_mnth), 2),\n  MAE_FE = round(MAE(wido$lifesatisfaction_gam_f, wido$m_lifesat_per_mnth), 2),\n  RMSE_FE = round(RMSE(wido$lifesatisfaction_gam_f, wido$m_lifesat_per_mnth), 2)\n)\n\n  R2_FE MAE_FE RMSE_FE\n1  0.33   0.28    0.38\n\n# Calculate R², MAE, and RMSE for the random effects predictions\ndata.frame(\n  R2_RE = round(R2(wido$lifesatisfaction_gam_r, wido$lifesatisfaction), 2),\n  MAE_RE = round(MAE(wido$lifesatisfaction_gam_r, wido$lifesatisfaction), 2),\n  RSME_RE = round(RMSE(wido$lifesatisfaction_gam_r, wido$lifesatisfaction), 2)\n)\n\n  R2_RE MAE_RE RSME_RE\n1  0.79   0.39    0.52\n\n\n\n\nCross-Validation\nTo assess the replicability of the model, perform cross-validation using the training and test datasets. For each training dataset, fit the model and compute performance metrics for the associated test dataset R², MAE, and RMSE.\n\n# Determine the lowest number of unique values of time in the training datasets to define k\nunique_timevalues_training_data &lt;- numeric(length(training_datasets))\n\nfor (i in 1:length(training_datasets)) {\n  training_data &lt;- training_datasets[[i]]\n  unique_timevalues_training_data[i] &lt;- n_distinct(training_data$mnths)\n}\n\nmin(unique_timevalues_training_data)\n\n[1] 318\n\n# Initialise vectors to store performance metrics\nR2_values &lt;- c()\nMAE_values &lt;- c()\nRMSE_values &lt;- c()\n\n# Perform cross-validation\nfor (i in 1:length(training_datasets)) {\n  # Get the current training and test dataset\n  training_data &lt;- training_datasets[[i]]\n  test_data &lt;- test_datasets[[i]]\n  \n  # Fit the model\n  gam &lt;- gam(lifesatisfaction ~ s(mnths, k = 317) + s(mnths, id, bs = \"fs\", m = 1), data = training_data, method = \"REML\")\n  \n  # Predict fixed effects\n  predictions &lt;- predict(gam, newdata = test_data, exclude = \"s(mnths,id)\")\n  \n  # Compute average test trajectory\n  test_data &lt;- test_data %&gt;%\n    group_by(mnths) %&gt;%\n    mutate(mean_ls = mean(lifesatisfaction, na.rm = TRUE))\n  \n  # Compute performance metrics\n  R2_value &lt;- R2(predictions, test_data$mean_ls)\n  RMSE_value &lt;- RMSE(predictions, test_data$mean_ls)\n  MAE_value &lt;- MAE(predictions, test_data$mean_ls)\n  \n  # Store the metrics\n  R2_values &lt;- c(R2_values, R2_value)\n  RMSE_values &lt;- c(RMSE_values, RMSE_value)\n  MAE_values &lt;- c(MAE_values, MAE_value)\n}\n\n# Compute average performance metrics (mean)\n  average_R2 &lt;- mean(R2_values)\n  average_MAE &lt;- mean(MAE_values)\n  average_RMSE &lt;- mean(RMSE_values)\n\n# Compute average performance metrics (SD)\n  sd_R2 &lt;- sd(R2_values)\n  sd_MAE &lt;- sd(MAE_values)\n  sd_RMSE &lt;- sd(RMSE_values)\n\n# Combine the mean and standard deviation into one data.frame\ncombined_metrics &lt;- data.frame(\n  Metric = c(\"R²\", \"MAE\", \"RMSE\"),\n  Mean = round(c(average_R2, average_MAE, average_RMSE), 2),\n  SD = round(c(sd_R2, sd_MAE, sd_RMSE), 2)\n)\n\n# Print the combined metrics\nprint(combined_metrics)\n\n  Metric Mean   SD\n1     R² 0.12 0.05\n2    MAE 0.58 0.08\n3   RMSE 0.76 0.12",
    "crumbs": [
      "Generalised Additive Model"
    ]
  },
  {
    "objectID": "CPA.html",
    "href": "CPA.html",
    "title": "Changepoint Analysis",
    "section": "",
    "text": "To illustrate changepoint analysis, we fit a linear-linear changepoint model.\nTo reproduce the results, it is necessary to prepare the data set, plot base, and training and test data sets, as outlined in the “Data Preparation” section.",
    "crumbs": [
      "Changepoint Analysis"
    ]
  },
  {
    "objectID": "CPA.html#preparation",
    "href": "CPA.html#preparation",
    "title": "Changepoint Analysis",
    "section": "Preparation",
    "text": "Preparation\n\nLoading Required Packages and Data\nLoad the necessary packages, data sets, and other supporting files. Each element serves a specific purpose:\n\ntidyverse: For data manipulation and visualisation.\nsegmented: To fit the changepoint model.\nlme4: Fitting a changepoint model in segmented requires a lme-object created using lme4.\ncaret: To compute model performance indices.\nplot_base: A pre-configured ggplot object for visualisation.\nTraining and Test Data sets: Required for cross-validation.\n\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(segmented)\nlibrary(lme4)\nlibrary(caret)\n\n# Load the data set\nload(\"data/wido.rdata\")\n\n# Load the pre-configured plot base\nplot_base &lt;- readRDS(\"objects/plot_base.rds\")\n\n# Load training and test datasets for cross-validation\ntraining_datasets &lt;- readRDS(\"objects/training_datasets.rds\")\ntest_datasets &lt;- readRDS(\"objects/test_datasets.rds\")",
    "crumbs": [
      "Changepoint Analysis"
    ]
  },
  {
    "objectID": "CPA.html#analysis",
    "href": "CPA.html#analysis",
    "title": "Changepoint Analysis",
    "section": "Analysis",
    "text": "Analysis\n\nFitting the Model\nFitting a changepoint model in segmented requires a lme-object created using lme4. Create a lme-object of the model, without the changepoint. Use this lme-object to fit the changepoint model. The comments in the code below indicate what should be filled in. Instead of “pdDiag” (= correlations between random effects are constrained to be 0), “pdSymm” (= random effects and their correlations are unconstrained), and “pdBlocked” (= specify which random effects can be correlated, and which correlations are constrained to be 0) are also possible. A starting value for the changepoint needs to be specified, but using bootstrap resampling mitigates sensitivity to starting values.\n\n# Create a linear mixed-effects model object\nlme_object &lt;- lme(lifesatisfaction ~ mnths,\n                  random = ~ mnths | id,\n                  data = wido)\n\n# Fit the changepoint model\ncp &lt;- segmented.lme(\n  lme_object,  # The linear mixed-effects model object\n  ~ mnths,     # A one-sided formula indicating the variable with a changepoint\n  random =     # A list of the random effects\n    list(id = pdDiag( ~ 1 + mnths + U + G0)), # U = the difference-in-slopes parameter; G0 = the changepoint\n  # Note that instead of \"pdDiag\" above, \"pdBlocked\" and \"pdSymm\" are also possible\n  psi = 0,     # Provide a starting value for the changepoint\n  control = seg.control( # Use bootstrap to mitigate potential sensitivity to starting values\n    display = F,\n    n.boot = 100,\n    seed = 123\n  )\n) \n\n# Display the summary of the model\nsummary(cp)\n\nSegmented mixed-effects model fit by REML\n       AIC      BIC    logLik\n  5409.571 5461.311 -2695.785\n Bootstrap restarting on 100 samples;  5 different solution(s)\n\nRandom effects:\n Formula: ~1 + mnths + U + G0 | id\n Structure: Diagonal\n        (Intercept)       mnths            U       G0  Residual\nStdDev:   0.8028599 0.005873525 0.0003828891 1.592956 0.6456952\n\nFixed effects:\n                 Value Std.Error   DF t-value p-value\n(Intercept)  4.680470 0.0620050 2111   75.48       0\n-- leftS:                                           \nmnths       -0.007652 0.0006958 2111  -11.00       0\n-- diffS:                                           \nU            0.015487 0.0011710 2111   13.23        \n-- break:                                           \nG0           5.368898 4.6777896 2111                \n psi.link = identity \n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-5.39537983 -0.49421689  0.06929748  0.57182047  3.49614881 \n\nNumber of Observations: 2322\nNumber of Groups: 208 \n\n# Display the slope estimates\nslope(cp)\n\n                   Est.       St.Err    t value     0.95.low      0.95.up\nleftSlope  -0.007651572 0.0006958491 -10.996022 -0.009015412 -0.006287733\nrightSlope  0.007835602 0.0010132926   7.732813  0.005849585  0.009821619\n\n# Compute confidence intervals for the model parameters\nintervals(cp$lme.fit, which = \"all\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                   lower         est.        upper\n(Intercept)  4.568734113  4.690331382  4.811928651\nmnths       -0.009651489 -0.008286868 -0.006922246\nU            0.013879689  0.016176031  0.018472374\nG0          -3.804660960  5.368897948 14.542456856\n\n Random Effects:\n  Level: id \n                       lower         est.        upper\nsd((Intercept)) 7.229097e-01 0.8028599098  0.891652219\nsd(mnths)       4.958933e-03 0.0058735248  0.006956798\nsd(U)           1.988277e-07 0.0003828891  0.737342242\nsd(G0)          5.594838e-02 1.5929562719 45.354478502\n\n Within-group standard error:\n    lower      est.     upper \n0.6253740 0.6456952 0.6666767",
    "crumbs": [
      "Changepoint Analysis"
    ]
  },
  {
    "objectID": "CPA.html#visualisation",
    "href": "CPA.html#visualisation",
    "title": "Changepoint Analysis",
    "section": "Visualisation",
    "text": "Visualisation\n\nBootstrapping Confidence Intervals\nUse bootstrapping to estimate the confidence intervals for the predicted values of the model. This provides a robust measure of uncertainty. Create custom functions to perform the bootstrap resampling.\n\n# For reproducibility\nset.seed(123)\n\n# Create a custom function to fit the model and generate predictions based on the estimated fixed effects\npredict_fun &lt;- function(data, mnths_vals) {\n  \n  # Create a linear mixed-effects model object\n  lme_object &lt;- lme(fixed = lifesatisfaction ~ mnths, random = ~mnths | id, data = data)\n  \n  # Apply the segmented mixed-effects model\n  cp_model &lt;- segmented.lme(\n    obj = lme_object,          \n    seg.Z = ~mnths, \n    random = list(id = pdDiag(~1 + mnths + U + G0)),\n    psi = 0,  \n    control = seg.control(display = F, n.boot = 0),\n    data = data\n  )\n  \n  # Create an empty vector to store predictions for the given mnths values\n  predictions &lt;- numeric(length(mnths_vals))\n  \n  # Predict the fixed effects for each level of mnths\n  for (i in 1:length(mnths_vals)) {\n    mnth &lt;- mnths_vals[i]\n    \n    # Use the breakpoint to compute predictions\n    predictions[i] &lt;- if_else(mnth &lt; cp_model$lme.fit$coefficients$fixed[[4]], \n                        (cp_model$lme.fit$coefficients$fixed[[1]] + \n                           (cp_model$lme.fit$coefficients$fixed[[2]] * mnth)), \n                        (cp_model$lme.fit$coefficients$fixed[[1]] + \n                           ((cp_model$lme.fit$coefficients$fixed[[2]] + cp_model$lme.fit$coefficients$fixed[[3]]) * mnth)))\n  }\n  \n  # Return the predicted fixed effects\n  return(predictions)\n}\n\n# Manual Bootstrap Process\nn_iter &lt;- 100  # Number of bootstrap iterations\n\n# Create an empty matrix to store the predictions\nbootstrap_predictions &lt;- matrix(NA, nrow = n_iter, ncol = length(seq(min(wido$mnths), max(wido$mnths), by = 1)))\n\n# Define a sequence of mnths values (the levels for which predictions are to be made)\nmnths_seq &lt;- seq(min(wido$mnths), max(wido$mnths), by = 1)\n\n# Loop over bootstrap iterations\nfor (i in 1:n_iter) {\n  # Resample the data with replacement\n  bootstrap_sample &lt;- wido[sample(nrow(wido), replace = TRUE), ]\n  \n  # Predict fixed effects for the resampled data based on the defined mnths sequence\n  bootstrap_predictions[i, ] &lt;- predict_fun(data = bootstrap_sample, mnths_vals = mnths_seq)\n}\n\n# The bootstrap_predictions matrix now contains the predictions for each iteration and mnths value\n\n# Calculate 95% confidence intervals from bootstrapped predictions for each mnths level\nci95 &lt;- apply(bootstrap_predictions, 2, quantile, probs = c(0.025, 0.975), na.rm = TRUE)\n\n# Store the lower and upper bounds in a new data frame that matches the mnths sequence\nbootci_results &lt;- data.frame(mnths = mnths_seq, lower_bound = ci95[1, ], upper_bound = ci95[2, ])\n\n\n\nPredicting Average and Individual Trajectories\nPredict both the population-level (fixed effects) and individual-level (random effects) trajectories of life satisfaction.\n\n# Predict population-level trajectories based on fixed effects\nwido &lt;- wido %&gt;%\n  mutate(lifesatisfaction_cp_f = if_else(mnths &lt; cp$lme.fit$coefficients$fixed[[4]],\n                             (cp$lme.fit$coefficients$fixed[[1]] + (cp$lme.fit$coefficients$fixed[[2]] * mnths)),\n                             (cp$lme.fit$coefficients$fixed[[1]] + ((cp$lme.fit$coefficients$fixed[[2]] + cp$lme.fit$coefficients$fixed[[3]]) * mnths))))\n\n# Obtain the individual-level predictions from the cp model object \nwido$lifesatisfaction_cp_r &lt;- fitted(cp)\n\n\n\nSelecting a Random Sample for Plotting\nFor better visualisation, select a random sample of individuals to display their individual trajectories.\n\n# For reproducibility\nset.seed(123)\n\n# Randomly sample 50 participants\nrsample_ids &lt;- sample(unique(wido$id), 50)\n\n# Filter the data to include only the randomly selected participants\nwido_rsample &lt;- wido %&gt;%\n  filter(id %in% rsample_ids)\n\n\n\nCreating the Plot\nCombine all elements to create the plot, which includes individual trajectories, the population trajectory, and the confidence interval of the population trajectory.\n\n# Create the plot using the pre-configured plot base\nplot_base +\n  geom_line(\n    data = wido_rsample,\n    aes(mnths, lifesatisfaction_cp_r, group = id),\n    color = \"grey70\",\n    linewidth = 0.4\n  ) +\n  geom_line(\n    data = wido,\n     aes(\n      x = mnths,\n      y = ifelse(mnths == round(cp$lme.fit$coefficients$fixed[[4]], 0), NA, lifesatisfaction_cp_f)\n      ),\n    color = \"firebrick4\",\n    linewidth = 1\n  ) +\n  geom_ribbon(\n    data = bootci_results,\n    aes(ymin = lower_bound, ymax = upper_bound, x = mnths),\n    alpha = 0.2,\n    fill = \"firebrick4\"\n  ) +\n  ggtitle(\"Changepoint Analysis\") +\n  theme(plot.title = element_text(size = 13, face = \"bold\"))",
    "crumbs": [
      "Changepoint Analysis"
    ]
  },
  {
    "objectID": "CPA.html#model-performance",
    "href": "CPA.html#model-performance",
    "title": "Changepoint Analysis",
    "section": "Model Performance",
    "text": "Model Performance\n\nEvaluating the Model\nAssess the model’s performance using the Bayesian Information Criterion (BIC), R-squared (R²), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).\n\n# Compute BIC for the fitted model\nround(BIC(cp), 2)\n\n[1] 5461.31\n\n# Calculate R², MAE, and RMSE for the fixed effects predictions\ndata.frame(\n  R2_FE = round(R2(wido$lifesatisfaction_cp_f, wido$m_lifesat_per_mnth), 2),\n  MAE_FE = round(MAE(wido$lifesatisfaction_cp_f, wido$m_lifesat_per_mnth), 2),\n  RMSE_FE = round(RMSE(wido$lifesatisfaction_cp_f, wido$m_lifesat_per_mnth), 2)\n)\n\n  R2_FE MAE_FE RMSE_FE\n1  0.17   0.35    0.48\n\n# Calculate R², MAE, and RMSE for the random effects predictions\ndata.frame(\n  R2_RE = round(R2(wido$lifesatisfaction_cp_r, wido$lifesatisfaction), 2),\n  MAE_RE = round(MAE(wido$lifesatisfaction_cp_r, wido$lifesatisfaction), 2),\n  RSME_RE = round(RMSE(wido$lifesatisfaction_cp_r, wido$lifesatisfaction), 2)\n)\n\n  R2_RE MAE_RE RSME_RE\n1   0.7   0.46    0.61\n\n\n\n\nCross-Validation\nTo assess the replicability of the model, perform cross-validation using the training and test datasets. For each training dataset, fit the model and compute performance metrics for the associated test dataset R², MAE, and RMSE.\n\n# Initialise vectors to store performance metrics\nR2_values &lt;- c()\nMAE_values &lt;- c()\nRMSE_values &lt;- c()\n\n# Loop over the datasets\nfor (i in 1:length(training_datasets)) {\n  # Get the current training and test dataset\n  training_data &lt;- training_datasets[[i]]\n  test_data &lt;- test_datasets[[i]]\n  \n  # Fit the initial linear mixed model\n  fit_lme &lt;- lme(lifesatisfaction ~ mnths, random = ~mnths | id, data = training_data)\n  \n  # Apply the segmented mixed-effects model\n  cp &lt;- segmented.lme(\n    fit_lme,            \n    ~mnths, \n    random = list(id = pdDiag(~1 + mnths + U + G0)),  # Adjust as needed based on your actual random effects\n    psi = 0,  # Initial breakpoint value for segmentation\n    control = seg.control(display = F, n.boot = 100, seed = 123)\n  )\n  \n  # Predict fixed effects from the segmented model\n  test_data &lt;- test_data %&gt;%\n    mutate(pred_cp_f = if_else(mnths &lt; cp$lme.fit$coefficients$fixed[[4]], \n                               (cp$lme.fit$coefficients$fixed[[1]] + (cp$lme.fit$coefficients$fixed[[2]]*mnths)), \n                               (cp$lme.fit$coefficients$fixed[[1]] + ((cp$lme.fit$coefficients$fixed[[2]] + cp$lme.fit$coefficients$fixed[[3]])*mnths))))\n  \n  # Compute average test trajectory\n  test_data &lt;- test_data %&gt;%\n    group_by(mnths) %&gt;%\n    mutate(mean_ls = mean(lifesatisfaction, na.rm = TRUE))\n  \n  # Compute performance metrics\n  R2_value &lt;- R2(test_data$pred_cp_f, test_data$mean_ls)\n  RMSE_value &lt;- RMSE(test_data$pred_cp_f, test_data$mean_ls)\n  MAE_value &lt;- MAE(test_data$pred_cp_f, test_data$mean_ls)\n  \n  # Store the metrics\n  R2_values &lt;- c(R2_values, R2_value)\n  RMSE_values &lt;- c(RMSE_values, RMSE_value)\n  MAE_values &lt;- c(MAE_values, MAE_value)\n}\n\n# Compute average performance metrics (mean)\n  average_R2 &lt;- mean(R2_values)\n  average_MAE &lt;- mean(MAE_values)\n  average_RMSE &lt;- mean(RMSE_values)\n\n# Compute average performance metrics (SD)\n  sd_R2 &lt;- sd(R2_values)\n  sd_MAE &lt;- sd(MAE_values)\n  sd_RMSE &lt;- sd(RMSE_values)\n\n# Combine the mean and standard deviation into one data.frame\ncombined_metrics &lt;- data.frame(\n  Metric = c(\"R²\", \"MAE\", \"RMSE\"),\n  Mean = round(c(average_R2, average_MAE, average_RMSE), 2),\n  SD = round(c(sd_R2, sd_MAE, sd_RMSE), 2)\n)\n\n# Print the combined metrics\nprint(combined_metrics)\n\n  Metric Mean   SD\n1     R² 0.07 0.07\n2    MAE 0.60 0.11\n3   RMSE 0.81 0.17",
    "crumbs": [
      "Changepoint Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelling Nonlinear Personality Change Surrounding Transitions",
    "section": "",
    "text": "These pages demonstrate how to apply six approaches to the statistical modelling of nonlinear change using R. They are intended to be used in conjunction with the explanations provided in the paper “Modelling Nonlinear Personality Change Surrounding Transitions: A Review of Statistical Approaches”.\nTo reproduce the results, it is necessary to prepare the data set, plot base, and training and test data sets, as outlined in the “Data Preparation” section.\nR version and package versions used:\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: \n\nMatrix products: \n\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] mvtnorm_1.3-2       rstan_2.32.6        StanHeaders_2.32.10\n [4] lgpr_1.2.4          tidygam_0.2.0       gratia_0.9.2       \n [7] mgcv_1.9-1          segmented_2.1-3     MASS_7.3-61        \n[10] boot_1.3-31         nlme_3.1-166        nls.multstart_1.3.0\n[13] caret_6.0-94        lattice_0.22-6      lmerTest_3.1-3     \n[16] lme4_1.1-35.5       Matrix_1.7-1        lubridate_1.9.3    \n[19] forcats_1.0.0       stringr_1.5.1       dplyr_1.1.4        \n[22] purrr_1.0.2         readr_2.1.5         tidyr_1.3.1        \n[25] tibble_3.2.1        ggplot2_3.5.1       tidyverse_2.0.0    \n[28] foreign_0.8-87"
  },
  {
    "objectID": "Lin.html",
    "href": "Lin.html",
    "title": "Linear Regression on a Transformed Time Variable",
    "section": "",
    "text": "To illustrate linear regression on a transformed time variable we fit a quadratic polynomial.\nTo reproduce the results, it is necessary to prepare the data set, plot base, and training and test data sets, as outlined in the “Data Preparation” section.",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "Lin.html#preparation",
    "href": "Lin.html#preparation",
    "title": "Linear Regression on a Transformed Time Variable",
    "section": "Preparation",
    "text": "Preparation\n\nLoading Required Packages and Data\nLoad the necessary packages, data sets, and other supporting files. Each element serves a specific purpose:\n\ntidyverse: For data manipulation and visualisation.\nlme4 and lmerTest: To fit and analyse mixed-effects models.\ncaret: To compute model performance indices.\nplot_base: A pre-configured ggplot object for visualisation.\nTraining and Test Data sets: Required for cross-validation.\n\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(caret)\n\n# Load the data set\nload(\"data/wido.rdata\")\n\n# Load the pre-configured plot base\nplot_base &lt;- readRDS(\"objects/plot_base.rds\")\n\n# Load training and test datasets for cross-validation\ntraining_datasets &lt;- readRDS(\"objects/training_datasets.rds\")\ntest_datasets &lt;- readRDS(\"objects/test_datasets.rds\")\n\n\n\nApplying an Orthogonal Polynomial\nTo avoid multicollinearity arising from using two terms of time (a linear and a quadratic term), we use an orthogonal polynomial. This ensures that the linear and quadratic time terms are uncorrelated. The poly() function generates two orthogonal terms: the linear and the quadratic components of time, stored in poly_time.\n\n# Apply orthogonal polynomial transformation to the time variable\nwido$poly_time &lt;- poly(wido$mnths, 2)",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "Lin.html#analysis",
    "href": "Lin.html#analysis",
    "title": "Linear Regression on a Transformed Time Variable",
    "section": "Analysis",
    "text": "Analysis\n\nFitting the Model\nFit the linear mixed-effects model using the transformed time variable (poly_time). This model includes both fixed effects for the linear and quadratic time terms and random effects for these terms to account for person-specific trajectories.\n\n# Fit the linear mixed-effects model\nlin &lt;- lmer(\n  lifesatisfaction ~ poly_time[,1] + poly_time[,2] + \n                     (poly_time[,1] + poly_time[,2] | id), \n  data = wido\n)\n\n# Display the summary of the model\nsummary(lin)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: lifesatisfaction ~ poly_time[, 1] + poly_time[, 2] + (poly_time[,  \n    1] + poly_time[, 2] | id)\n   Data: wido\n\nREML criterion at convergence: 5512.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.4176 -0.4947  0.0778  0.5705  3.3371 \n\nRandom effects:\n Groups   Name           Variance Std.Dev. Corr       \n id       (Intercept)      0.6131  0.7830             \n          poly_time[, 1] 442.7048 21.0406   0.15      \n          poly_time[, 2]  66.0257  8.1256  -0.23  0.11\n Residual                  0.4358  0.6601             \nNumber of obs: 2322, groups:  id, 208\n\nFixed effects:\n                Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)      4.95087    0.05902 207.34316  83.887  &lt; 2e-16 ***\npoly_time[, 1]  -8.94865    1.89670 102.63469  -4.718 7.54e-06 ***\npoly_time[, 2]   5.93670    1.46277  69.70255   4.059 0.000127 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) p_[,1]\npoly_tm[,1] 0.109        \npoly_tm[,2] 0.148  0.086 \n\n# Compute confidence intervals for the model parameters\nround(confint(lin), 2)\n\nComputing profile confidence intervals ...\n\n\n                2.5 % 97.5 %\n.sig01           0.70   0.87\n.sig02          -0.04   0.34\n.sig03          -0.89   0.15\n.sig04          17.36  24.92\n.sig05          -0.84   0.70\n.sig06           1.05  12.74\n.sigma           0.64   0.68\n(Intercept)      4.83   5.07\npoly_time[, 1] -13.08  -4.76\npoly_time[, 2]   2.49   9.37",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "Lin.html#visualisation",
    "href": "Lin.html#visualisation",
    "title": "Linear Regression on a Transformed Time Variable",
    "section": "Visualisation",
    "text": "Visualisation\n\nBootstrapping Confidence Intervals\nUse bootstrapping to estimate the confidence intervals for the predicted values of the model. This provides a robust measure of uncertainty.\n\n# For reproducibility\nset.seed(123)\n\n# Bootstrapping for confidence intervals of the predictions\nboot_results &lt;- bootMer(\n  lin, \n  FUN = function(x) predict(x, newdata = wido, re.form = NA), \n  nsim = 1000\n)\n\n# Extract the 95% confidence intervals from the bootstrapped results\nci &lt;- apply(boot_results$t, 2, quantile, probs = c(0.025, 0.975))\n\n# Assign the lower and upper bounds to the data\nwido$lower_bound &lt;- ci[1, ]\nwido$upper_bound &lt;- ci[2, ]\n\n\n\nPredicting Average and Individual Trajectories\nPredict both the population-level (fixed effects) and individual-level (random effects) trajectories of life satisfaction.\n\n# Predict population-level trajectories based on fixed effects\nwido$lifesatisfaction_lin_f &lt;- predict(lin, newdata = wido, re.form = NA)\n\n# Predict individual-level trajectories based on random effects\nwido$lifesatisfaction_lin_r &lt;- predict(lin, newdata = wido, re.form = NULL, allow.new.levels = TRUE)\n\n\n\nSelecting a Random Sample for Plotting\nFor better visualisation, select a random sample of individuals to display their individual trajectories.\n\n# For reproducibility\nset.seed(123)\n\n# Randomly sample 50 participants\nrsample_ids &lt;- sample(unique(wido$id), 50)\n\n# Filter the data to include only the randomly selected participants\nwido_rsample &lt;- wido %&gt;%\n  filter(id %in% rsample_ids)\n\n\n\nCreating the Plot\nCombine all elements to create the plot, which includes individual trajectories, the population trajectory, and the confidence interval of the population trajectory.\n\n# Create the plot using the pre-configured plot base\nplot_base + \n  geom_line(\n    data = wido_rsample, \n    aes(x = mnths, y = lifesatisfaction_lin_r, group = id), \n    color = \"grey70\", linewidth = 0.4\n  ) +\n  geom_ribbon(\n    data = wido, \n    aes(x = mnths, ymin = lower_bound, ymax = upper_bound), \n    fill = \"firebrick4\", alpha = 0.2\n  ) +\n  geom_line(\n    data = wido, \n    aes(x = mnths, y = lifesatisfaction_lin_f), \n    color = \"firebrick4\", linewidth = 1\n  ) +\n  ggtitle(\"Linear Regression with a Quadratic Transformation\") +\n  theme(plot.title = element_text(size = 13, face = \"bold\"))",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "Lin.html#model-performance",
    "href": "Lin.html#model-performance",
    "title": "Linear Regression on a Transformed Time Variable",
    "section": "Model Performance",
    "text": "Model Performance\n\nEvaluating the Model\nAssess the model’s performance using the Bayesian Information Criterion (BIC), R-squared (R²), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).\n\n# Compute BIC for the fitted model\nround(BIC(lin), 2)\n\n[1] 5590.19\n\n# Calculate R², MAE, and RMSE for the fixed effects predictions\ndata.frame(\n  R2_FE = round(R2(wido$lifesatisfaction_lin_f, wido$m_lifesat_per_mnth), 2),\n  MAE_FE = round(MAE(wido$lifesatisfaction_lin_f, wido$m_lifesat_per_mnth), 2),\n  RMSE_FE = round(RMSE(wido$lifesatisfaction_lin_f, wido$m_lifesat_per_mnth), 2)\n)\n\n  R2_FE MAE_FE RMSE_FE\n1  0.14   0.33    0.43\n\n# Calculate R², MAE, and RMSE for the random effects predictions\ndata.frame(\n  R2_RE = round(R2(wido$lifesatisfaction_lin_r, wido$lifesatisfaction), 2),\n  MAE_RE = round(MAE(wido$lifesatisfaction_lin_r, wido$lifesatisfaction), 2),\n  RSME_RE = round(RMSE(wido$lifesatisfaction_lin_r, wido$lifesatisfaction), 2)\n)\n\n  R2_RE MAE_RE RSME_RE\n1   0.7   0.46    0.61\n\n\n\n\nCross-Validation\nTo assess the replicability of the model, perform cross-validation using the training and test data sets. For each training data set, fit the model and compute performance metrics for the associated test data set R², MAE, and RMSE.\n\n# Initialise vectors to store performance metrics\nR2_values &lt;- c()\nMAE_values &lt;- c()\nRMSE_values &lt;- c()\n\n# Perform cross-validation\nfor (i in seq_along(training_datasets)) {\n  train_data &lt;- training_datasets[[i]]\n  test_data &lt;- test_datasets[[i]]\n  \n  # Apply polynomial transformation to time variable\n  train_data$poly_time &lt;- poly(train_data$mnths, 2)\n  test_data$poly_time &lt;- poly(test_data$mnths, 2)\n  \n  # Fit the linear mixed-effects model on training data\n  lin &lt;- lmer(\n    lifesatisfaction ~ poly_time[,1] + poly_time[,2] + \n                       (poly_time[,1] + poly_time[,2] | id), \n    data = train_data\n  )\n  \n  # Make predictions on the test data\n  test_predictions &lt;- predict(lin, newdata = test_data, re.form = NA)\n  \n  # Compute average trajectory in the test data\n  test_data &lt;- test_data %&gt;%\n    group_by(mnths) %&gt;%\n    mutate(m_lifesat_per_mnth = mean(lifesatisfaction, na.rm = TRUE))\n  \n  # Calculate performance metrics\n  R2_values &lt;- c(R2_values, R2(test_predictions, test_data$m_lifesat_per_mnth))\n  MAE_values &lt;- c(MAE_values, MAE(test_predictions, test_data$m_lifesat_per_mnth))\n  RMSE_values &lt;- c(RMSE_values, RMSE(test_predictions, test_data$m_lifesat_per_mnth))\n}\n\n# Compute average performance metrics (mean)\n  average_R2 &lt;- mean(R2_values)\n  average_MAE &lt;- mean(MAE_values)\n  average_RMSE &lt;- mean(RMSE_values)\n\n# Compute average performance metrics (SD)\n  sd_R2 &lt;- sd(R2_values)\n  sd_MAE &lt;- sd(MAE_values)\n  sd_RMSE &lt;- sd(RMSE_values)\n\n# Combine the mean and standard deviation into one data.frame\ncombined_metrics &lt;- data.frame(\n  Metric = c(\"R²\", \"MAE\", \"RMSE\"),\n  Mean = round(c(average_R2, average_MAE, average_RMSE), 2),\n  SD = round(c(sd_R2, sd_MAE, sd_RMSE), 2)\n)\n\n# Print the combined metrics\nprint(combined_metrics)\n\n  Metric Mean   SD\n1     R² 0.06 0.04\n2    MAE 0.63 0.07\n3   RMSE 0.83 0.10",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "Nonlin.html",
    "href": "Nonlin.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "To illustrate nonlinear regression we use a model that combines a Gaussian and a logistic function.\nTo reproduce the results, it is necessary to prepare the data set, plot base, and training and test data sets, as outlined in the “Data Preparation” section.",
    "crumbs": [
      "Nonlinear Regression"
    ]
  },
  {
    "objectID": "Nonlin.html#preparation",
    "href": "Nonlin.html#preparation",
    "title": "Nonlinear Regression",
    "section": "Preparation",
    "text": "Preparation\n\nLoading Required Packages and Data\nLoad the necessary packages, data sets, and other supporting files. Each element serves a specific purpose:\n\ntidyverse: For data manipulation and visualisation.\nnls.multstart: To identify reasonable starting values for the nonlinear regression.\nnlme: To fit the nonlinear regression model.\nboot: To calculate the bootstrapped 95% CI.\ncaret: To compute model performance indices.\nplot_base: A pre-configured ggplot object for visualisation.\nTraining and Test Data sets: Required for cross-validation.\n\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(nls.multstart)\nlibrary(nlme)\nlibrary(boot)\nlibrary(caret)\n\n# Load the data set\nload(\"data/wido.rdata\")\n\n# Load the pre-configured plot base\nplot_base &lt;- readRDS(\"objects/plot_base.rds\")\n\n# Load training and test datasets for cross-validation\ntraining_datasets &lt;- readRDS(\"objects/training_datasets.rds\")\ntest_datasets &lt;- readRDS(\"objects/test_datasets.rds\")\n\n\n\nCreate Nonlinear Function\nCreate a custom nonlinear function by combining the formula for a Gaussian function and the formula for a logistic function using a plus sign.\n\n# Define the nonlinear function\nnonlinfunc &lt;- function(mnths, bas, amp, wid, cen, newequi) {\n  return ((bas + amp * exp(-(mnths - cen)^2 / (2 * wid^2))) + (newequi*(1 / (1 + exp(-(mnths - cen) / wid)))\n  ))\n}",
    "crumbs": [
      "Nonlinear Regression"
    ]
  },
  {
    "objectID": "Nonlin.html#analysis",
    "href": "Nonlin.html#analysis",
    "title": "Nonlinear Regression",
    "section": "Analysis",
    "text": "Analysis\n\nIdentify Starting Values\nWe fit the nonlinear regression model including random effects in nlme. Because this requires starting values for the parameters, we estimate the model without random effects, using nls_multstart. This function repeatedly fits the model, each time using different starting values. The results of the model without random effects will inform the specification of starting values for the model with random effects.\n\n# Fit the model without random effects, specify reasonable limits for the starting values of the parameters\nnonlin_norandomeffects &lt;- nls_multstart(lifesatisfaction ~ nonlinfunc(mnths, bas, amp, wid, cen, newequi), \n                         data = wido,\n                         lower=c(bas=-7, amp=-7, wid=-200, cen=-200, newequi=-7),\n                         upper=c(bas=7, amp=7, wid=200, cen=200, newequi=7),\n                         start_lower = c(bas=-7, amp=-7, wid=-200, cen=-200, newequi=-7),\n                         start_upper = c(bas=7, amp=7, wid=200, cen=200, newequi=7),\n                         iter = 500,\n                         supp_errors = \"Y\")\n\nsummary(nonlin_norandomeffects)\n\n\nFormula: lifesatisfaction ~ nonlinfunc(mnths, bas, amp, wid, cen, newequi)\n\nParameters:\n         Estimate Std. Error t value Pr(&gt;|t|)    \nbas       4.91869    0.04360 112.815  &lt; 2e-16 ***\namp      -0.67021    0.08010  -8.367  &lt; 2e-16 ***\nwid     -10.85986    1.62611  -6.678 3.01e-11 ***\ncen       5.01145    1.61313   3.107  0.00192 ** \nnewequi   0.28449    0.05627   5.056 4.62e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.081 on 2317 degrees of freedom\n\nNumber of iterations to convergence: 51 \nAchieved convergence tolerance: 1.49e-08\n\n\n\n\nFitting the Model\nWe use these results to specify starting values for the model with random effects. We add random effects for the “baseline”- and “new equilibrium”-parameters.\n\n# Fit the model with the identified starting values and random effects for baseline and new equilibrium\nnonlin_randomeffects_bas_newequi &lt;- nlme(lifesatisfaction ~ nonlinfunc(mnths, bas, amp, wid, cen, newequi), \n                                      data = wido,\n                                      fixed= bas + amp + wid + cen + newequi ~ 1, \n                                      random = bas + newequi ~ 1, \n                                      groups = ~ id,\n                                      start = c(bas=5.2, amp=-0.7, wid=10.9, cen=5.0, newequi=-0.3))\n\nsummary(nonlin_randomeffects_bas_newequi)\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: lifesatisfaction ~ nonlinfunc(mnths, bas, amp, wid, cen, newequi) \n  Data: wido \n       AIC     BIC    logLik\n  5307.388 5359.14 -2644.694\n\nRandom effects:\n Formula: list(bas ~ 1, newequi ~ 1)\n Level: id\n Structure: General positive-definite, Log-Cholesky parametrization\n         StdDev    Corr  \nbas      0.8598767 bas   \nnewequi  0.9276880 -0.384\nResidual 0.6248258       \n\nFixed effects:  bas + amp + wid + cen + newequi ~ 1 \n            Value Std.Error   DF   t-value p-value\nbas      5.167437 0.0657563 2110  78.58468   0e+00\namp     -0.716649 0.0591928 2110 -12.10703   0e+00\nwid      6.539394 0.5826737 2110  11.22308   0e+00\ncen      2.513732 0.6178153 2110   4.06874   0e+00\nnewequi -0.315205 0.0777650 2110  -4.05330   1e-04\n Correlation: \n        bas    amp    wid    cen   \namp     -0.078                     \nwid      0.103  0.336              \ncen     -0.068 -0.048  0.241       \nnewequi -0.437 -0.082 -0.011  0.157\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.89041434 -0.48124295  0.06419164  0.54053713  3.50118182 \n\nNumber of Observations: 2322\nNumber of Groups: 208 \n\nintervals(nonlin_randomeffects_bas_newequi)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n             lower       est.      upper\nbas      5.0386217  5.1674368  5.2962518\namp     -0.8326063 -0.7166491 -0.6006918\nwid      5.3979505  6.5393944  7.6808384\ncen      1.3034460  2.5137316  3.7240172\nnewequi -0.4675448 -0.3152051 -0.1628653\n\n Random Effects:\n  Level: id \n                      lower       est.      upper\nsd(bas)           0.7682378  0.8598767  0.9624467\nsd(newequi)       0.8057144  0.9276880  1.0681267\ncor(bas,newequi) -0.5197132 -0.3835289 -0.2283371\n\n Within-group standard error:\n    lower      est.     upper \n0.6052182 0.6248258 0.6450687",
    "crumbs": [
      "Nonlinear Regression"
    ]
  },
  {
    "objectID": "Nonlin.html#visualisation",
    "href": "Nonlin.html#visualisation",
    "title": "Nonlinear Regression",
    "section": "Visualisation",
    "text": "Visualisation\n\nBootstrapping Confidence Intervals\nUse bootstrapping to estimate the confidence intervals for the predicted values of the model. This provides a robust measure of uncertainty.\n\n# For reproducibility\nset.seed(123)\n\n# To avoid convergence issues, we adapt the control values for the nlme fit (increase iterations, etc.)\ncontrol_options &lt;- nlmeControl(\n  maxIter = 2000,    # Increase max number of iterations\n  pnlsMaxIter = 500, # Increase the max number of iterations for the PNLS step\n  msMaxIter = 2000,  # Increase the maximum iterations for the optimization step\n    pnlsTol = 0.1,   # Relax tolerance for PNLS step\n)\n\n# Define a function to refit the nonlinear mixed-effects model and generate predictions\npredict_fun &lt;- function(data, indices) {\n  # Resample the data using the bootstrap indices\n  boot_data &lt;- data[indices, ]\n  \n  # Refit the nonlinear mixed-effects model on the resampled data\n  boot_model &lt;- nlme(\n    lifesatisfaction ~ nonlinfunc(mnths, bas, amp, wid, cen, newequi), \n    data = boot_data,\n    fixed = bas + amp + wid + cen + newequi ~ 1, \n    random = bas + newequi ~ 1, \n    groups = ~ id,\n    start = c(bas=5.2, amp=-0.7, wid=10.9, cen=5.0, newequi=-0.3),\n    control = control_options  \n  )\n  \n  # Predict on the original dataset\n  # Predict using only fixed effects (set level = 0)\n  return(predict(boot_model, newdata = data, level = 0))\n}\n\n# Perform the bootstrap\nboot_results &lt;- boot(data = wido, statistic = predict_fun, R = 1000, sim = \"ordinary\")\n\n# Calculate 95% confidence intervals from bootstrapped results\nci &lt;- apply(boot_results$t, 2, quantile, probs = c(0.025, 0.975), na.rm = T)\n\n# Assign the results to the original data frame\nwido$lower_bound &lt;- ci[1, ]\nwido$upper_bound &lt;- ci[2, ]\n\n\n\nPredicting Average and Individual Trajectories\nPredict both the population-level (fixed effects) and individual-level (random effects) trajectories of life satisfaction.\n\n# Predict population-level trajectories based on fixed effects\nwido$lifesatisfaction_nl_fix &lt;- predict(nonlin_randomeffects_bas_newequi, wido, level = 0)\n\n# Predict individual-level trajectories based on random effects\nwido$lifesatisfaction_nl_rand &lt;- predict(nonlin_randomeffects_bas_newequi, newdata = wido)\n\n\n\nSelecting a Random Sample for Plotting\nFor better visualisation, select a random sample of individuals to display their individual trajectories.\n\n# For reproducibility\nset.seed(123)\n\n# Randomly sample 50 participants\nrsample_ids &lt;- sample(unique(wido$id), 50)\n\n# Filter the data to include only the randomly selected participants\nwido_rsample &lt;- wido %&gt;%\n  filter(id %in% rsample_ids)\n\n\n\nCreating the Plot\nCombine all elements to create the plot, which includes individual trajectories, the population trajectory, and the confidence interval of the population trajectory.\n\n# Create the plot using the pre-configured plot base\nplot_base + \n  geom_line(\n    data = wido_rsample, \n    aes(x = mnths, y = lifesatisfaction_nl_rand, group = id), \n    color = \"grey70\", linewidth = 0.4\n  ) +\n  geom_ribbon(\n    data = wido, \n    aes(x = mnths, ymin = lower_bound, ymax = upper_bound), \n    fill = \"firebrick4\", alpha = 0.2\n  ) +\n  geom_line(\n    data = wido, \n    aes(x = mnths, y = lifesatisfaction_nl_fix), \n    color = \"firebrick4\", linewidth = 1\n  ) +\n  ggtitle(\"Nonlinear Regression: Gaussian + \\nLogistic Function\") +\n  theme(plot.title = element_text(size = 13, face = \"bold\"))",
    "crumbs": [
      "Nonlinear Regression"
    ]
  },
  {
    "objectID": "Nonlin.html#model-performance",
    "href": "Nonlin.html#model-performance",
    "title": "Nonlinear Regression",
    "section": "Model Performance",
    "text": "Model Performance\n\nEvaluating the Model\nAssess the model’s performance using the Bayesian Information Criterion (BIC), R-squared (R²), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).\n\n# Compute BIC for the fitted model\nround(BIC(nonlin_randomeffects_bas_newequi), 2)\n\n[1] 5359.14\n\n# Calculate R², MAE, and RMSE for the fixed effects predictions\ndata.frame(\n  R2_FE = round(R2(wido$lifesatisfaction_nl_fix, wido$m_lifesat_per_mnth), 2),\n  R2_FE = round(MAE(wido$lifesatisfaction_nl_fix, wido$m_lifesat_per_mnth), 2),\n  R2_FE = round(RMSE(wido$lifesatisfaction_nl_fix, wido$m_lifesat_per_mnth), 2)\n)\n\n  R2_FE R2_FE.1 R2_FE.2\n1  0.28    0.29    0.39\n\n# Calculate R², MAE, and RMSE for the random effects predictions\ndata.frame(\n  R2_RE = round(R2(wido$lifesatisfaction_nl_rand, wido$lifesatisfaction), 2),\n  R2_RE = round(MAE(wido$lifesatisfaction_nl_rand, wido$lifesatisfaction), 2),\n  R2_RE = round(RMSE(wido$lifesatisfaction_nl_rand, wido$lifesatisfaction), 2)\n)\n\n  R2_RE R2_RE.1 R2_RE.2\n1  0.73    0.43    0.58\n\n\n\n\nCross-Validation\nTo assess the replicability of the model, perform cross-validation using the training and test data sets. For each training data set, fit the model and compute performance metrics for the associated test data set R², MAE, and RMSE.\n\n# Initialize vectors to store the performance metrics\nR2_values &lt;- c()\nRMSE_values &lt;- c()\nMAE_values &lt;- c()\n\n# Perform the cross-validation\nfor (i in 1:length(training_datasets)) {\n  # Get the current training and test data set\n  training_data &lt;- training_datasets[[i]]\n  test_data &lt;- test_datasets[[i]]\n  \n  # Fit the nonlinear model using nlme\n  nlme_model &lt;- nlme(\n    lifesatisfaction ~ nonlinfunc(mnths, bas, amp, wid, cen, newequi), \n    data = training_data,\n    fixed = bas + amp + wid + cen + newequi ~ 1, \n    random = bas + newequi ~ 1 | id,\n    start = c(bas=5.2, amp=-0.7, wid=10.9, cen=5.0, newequi=-0.3),\n  )\n  \n  # Predict fixed effects\n  predictions &lt;- predict(nlme_model, newdata = test_data, level = 0)\n  \n  # Compute average test trajectory\n  test_data &lt;- test_data %&gt;%\n    group_by(mnths) %&gt;%\n    mutate(mean_ls = mean(lifesatisfaction, na.rm = TRUE))\n  \n  # Compute performance metrics\n  R2_value &lt;- R2(predictions, test_data$mean_ls)\n  RMSE_value &lt;- RMSE(predictions, test_data$mean_ls)\n  MAE_value &lt;- MAE(predictions, test_data$mean_ls)\n  \n  # Store the metrics\n  R2_values &lt;- c(R2_values, R2_value)\n  RMSE_values &lt;- c(RMSE_value, RMSE_value)\n  MAE_values &lt;- c(MAE_values, MAE_value)\n}\n\n# Compute average performance metrics (mean)\n  average_R2 &lt;- mean(R2_values)\n  average_MAE &lt;- mean(MAE_values)\n  average_RMSE &lt;- mean(RMSE_values)\n\n# Compute average performance metrics (SD)\n  sd_R2 &lt;- sd(R2_values)\n  sd_MAE &lt;- sd(MAE_values)\n  sd_RMSE &lt;- sd(RMSE_values)\n\n# Combine the mean and standard deviation into one data.frame\ncombined_metrics &lt;- data.frame(\n  Metric = c(\"R²\", \"MAE\", \"RMSE\"),\n  Mean = round(c(average_R2, average_MAE, average_RMSE), 2),\n  SD = round(c(sd_R2, sd_MAE, sd_RMSE), 2)\n)\n\n# Print the combined metrics\nprint(combined_metrics)\n\n  Metric Mean   SD\n1     R² 0.10 0.02\n2    MAE 0.59 0.07\n3   RMSE 0.74 0.00",
    "crumbs": [
      "Nonlinear Regression"
    ]
  },
  {
    "objectID": "preparation.html",
    "href": "preparation.html",
    "title": "Data Preparation",
    "section": "",
    "text": "This script prepares the data for analysis and creates objects for the visualisation and cross-validation.\n\nLoad Required Packages\n\n# Load necessary R packages\nlibrary(foreign)   # For reading SPSS files\nlibrary(tidyverse) # For data manipulation and cleaning\n\n\n\nDownload Data\nCreate folders to organise the data files.\n\ndir.create(\"data\")\ndir.create(\"data/back\") #For the monthly background variables\ndir.create(\"data/pers\") #For the personality questionnaires\n\nDownload the monthly background variables files (‘avars_(…).sav’) from 2007 to 2023 from the LISS website and save them in the folder data/back. Download the personality questionnaires (‘cp_(…).sav’) from 2008 to 2023 from LISS and save them in the folder data/pers.\n\n\nLoad and Clean Data\nLoad these files in R.\n\n# List all .sav files in the 'data/back' directory\nfiles &lt;- list.files(path = \"data/back/\", pattern = \"*.sav\", full.names = TRUE)\n\n# Combine all the files into one dataset 'back.all'\nback.all &lt;- do.call('bind_rows', lapply(files, function(x) read.spss(x, use.value.labels = FALSE, to.data.frame = TRUE)))\n\n\n# Load personality data for each wave (2008 to 2023)\npe08  &lt;- read.spss('data/pers/cp08a_1p_EN.sav', use.value.labels=F, to.data.frame=T)\npe09  &lt;- read.spss('data/pers/cp09b_1.0p_EN.sav', use.value.labels=F, to.data.frame=T)\npe10  &lt;- read.spss('data/pers/cp10c_1.0p_EN.sav', use.value.labels=F, to.data.frame=T)\npe11  &lt;- read.spss('data/pers/cp11d_1.0p_EN.sav', use.value.labels=F, to.data.frame=T)\npe12  &lt;- read.spss('data/pers/cp12e_1.0p_EN.sav', use.value.labels=F, to.data.frame=T)\npe13  &lt;- read.spss('data/pers/cp13f_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\npe14  &lt;- read.spss('data/pers/cp14g_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\npe15  &lt;- read.spss('data/pers/cp15h_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\npe17  &lt;- read.spss('data/pers/cp17i_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\npe18  &lt;- read.spss('data/pers/cp18j_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\npe19  &lt;- read.spss('data/pers/cp19k_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\npe20  &lt;- read.spss('data/pers/cp20l_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\npe21  &lt;- read.spss('data/pers/cp21m_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\npe22  &lt;- read.spss('data/pers/cp22n_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\npe23  &lt;- read.spss('data/pers/cp23o_EN_1.0p.sav', use.value.labels=F, to.data.frame=T)\n\nPrepare the personality questionnaires for merging by renaming them consistently and removing unnecessary columns.\n\n# Make the names uniform by removing the wave qualifier (e.g., \"08a\")\nnames(pe08) &lt;- gsub(\"08a\", \"\", names(pe08))\nnames(pe09) &lt;- gsub(\"09b\", \"\", names(pe09))\nnames(pe10) &lt;- gsub(\"10c\", \"\", names(pe10))\nnames(pe11) &lt;- gsub(\"11d\", \"\", names(pe11))\nnames(pe12) &lt;- gsub(\"12e\", \"\", names(pe12))\nnames(pe13) &lt;- gsub(\"13f\", \"\", names(pe13))\nnames(pe14) &lt;- gsub(\"14g\", \"\", names(pe14))\nnames(pe15) &lt;- gsub(\"15h\", \"\", names(pe15))\nnames(pe17) &lt;- gsub(\"17i\", \"\", names(pe17))\nnames(pe18) &lt;- gsub(\"18j\", \"\", names(pe18))\nnames(pe19) &lt;- gsub(\"19k\", \"\", names(pe19))\nnames(pe20) &lt;- gsub(\"20l\", \"\", names(pe20))\nnames(pe21) &lt;- gsub(\"21m\", \"\", names(pe21))\nnames(pe22) &lt;- gsub(\"22n\", \"\", names(pe22))\nnames(pe23) &lt;- gsub(\"23o\", \"\", names(pe23))\n\n# Remove variables indicating date and duration of the questionnaire, \n# Because we don't need them and otherwise they cause trouble with merging as their variable type differs per wave   \npe08 &lt;- pe08 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe09 &lt;- pe09 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe10 &lt;- pe10 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe11 &lt;- pe11 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe12 &lt;- pe12 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe13 &lt;- pe13 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe14 &lt;- pe14 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe15 &lt;- pe15 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe17 &lt;- pe17 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe18 &lt;- pe18 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe19 &lt;- pe19 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe20 &lt;- pe20 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe21 &lt;- pe21 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe22 &lt;- pe22 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\npe23 &lt;- pe23 %&gt;% dplyr::select(-c(cp189, cp190, cp191, cp192, cp193))\n\n# Merge them all together\n# To receive one file including all waves for the personality data\npe.all &lt;- bind_rows(pe08, pe09, pe10, pe11, pe12, pe13, pe14, pe15, pe17, pe18, pe19, pe20, pe21, pe22, pe23)\n\n# Rename \"cp_m\" into \"wave\" to be consistent with \"back.all\"\nnames(pe.all)[3] &lt;- c(\"wave\")\n\n# Remove everything besides of participant ID (\"nomem_encr\"), Wave and Life Satisfaction items from data set\npe.all &lt;- pe.all %&gt;% dplyr::select(c(nomem_encr, wave, cp014, cp015, cp016, cp017, cp018))\n\n\n\nMerge Background and Personality Data\nMerge the background data (back.all) with the personality data (pe.all) on the nomem_encr (participant ID) and wave variables.\n\n# Merge background data and personality data\ndata &lt;- list(back.all, pe.all) %&gt;% reduce(full_join, by=c('nomem_encr', 'wave'))\n\n\n\nCompute Life Satisfaction Scores\nCompute life satisfaction scores by averaging responses from its five items (cp014 to cp018). Higher scores indicate more life satisfaction.\n\n# Compute mean life satisfaction for each row\ndata &lt;- data %&gt;%\n  rowwise() %&gt;%\n  mutate(lifesatisfaction = mean(c(cp014, cp015, cp016, cp017, cp018)))\n\n\n\nCreate Timeline Variable\nCreate variables for year and month of measurement, and a continuous timeline of measurements.\n\n# Create year and month variables\ndata$year &lt;- as.numeric(substr(data$wave, 1,4))\ndata$month &lt;- as.numeric(substr(data$wave, 5,6))\n\n# Create a continuous time axis (timeline in months)\n# NOTE: as the survey started in november 2007, this timeline is not completely correct yet \ndata$timeline &lt;- (data$year-2008)*12 + data$month\n\n# Adjust timeline to start from the correct point in time (November 2007)\ndata$timeline &lt;- data$timeline + 2\ndata &lt;- data[order(data$nomem_encr, data$wave),]\n\n# Remove objects from environment we do not need anymore\nrm(list = setdiff(ls(), \"data\"))\n\n\n\nCreate Widowhood Transition Variable\nCreate a transition variable that indicates when a participant experienced widowhood, based on a marital status change (“burgstat”). To do so, first create a variable that indicates a person’s previous marital status (from the previous wave).\n\n# Create a parallel dataset that consists of past data\n\n# Order data: rearrange data ascending based on nomem_encr (=participant id)\ndata &lt;- data[order(data$nomem_encr, data$wave),]\n\n# Create a variable that indicates the number of waves that a person has contributed (including current wave)\ndata$k &lt;- ave(data$wave, data$nomem_encr, FUN = seq_along)\n\n# Mirror the dataset\npastdata &lt;- data\n\n# Select the relevant variables\npastdata &lt;- pastdata[,c(\"nomem_encr\", \"k\", \"burgstat\")]\n\n# Add one to the wave number\npastdata$k.pa &lt;- pastdata$k\n\n# The following trick (+1) ensures that row k = 10 of \"data\" is combined with row 9 (k = 9 + 1 = 10) of \"pastdata\", which is therefore the \"past\"\npastdata$k &lt;- pastdata$k + 1 \n\n# Rename the variables to indicate the past\nnames(pastdata) &lt;- c(\"nomem_encr\", \"k\", \"burgstat.pa\", \"k.pa\")\n\n# Create an additional \"past id variable\", as a check (to check whether there are no \"nomadic\" lags)\npastdata$nomem_encr.pa &lt;- pastdata$nomem_encr\n\n# Merge files, exclude the highest lag of the \"pastdata\" file via \"all.x = T\" \n# (because the last wave cannot be a lag)\ndata &lt;- merge(data, pastdata, by=c(\"nomem_encr\", \"k\"), all.x=T)\n\n# Remove pastdata, because we do not need it anymore\nrm(pastdata)\n\nCreate the widowhood transition variable.\n\n# Create a variable indicating when a participant became widowed\ndata &lt;- data %&gt;%\n  mutate(wido.ev = ifelse(\n    nomem_encr == nomem_encr.pa & burgstat == 4 & burgstat.pa == 1,\n    1,\n    0\n  ))\n\n\n\nCreate Time Relative To Transition Variable\nCreate a variable indicating the timing of measurements relative to the transition.\n\n# Create variables for the exact time of widowhood event (year and month)\ndata &lt;- data %&gt;% \n  group_by(nomem_encr) %&gt;% \n  mutate(wido_time = ifelse(wido.ev == 1, paste0(year, month), NA)) %&gt;%\n  mutate(wido_time =  ifelse(any(!is.na(wido_time)), max(wido_time, na.rm = TRUE), NA))\n\n# Split 'wido_time' into year and month components\ndata$wido.year &lt;- as.numeric(substr(data$wido_time, 1,4))\ndata$wido.month &lt;- as.numeric(substr(data$wido_time, 5,6))\n\n# Create variable indicating the timing of measurements relative to the widowhood transition\n# (\"mnths\" because it is the timing of measurements relative to widowhood in months)\ndata$mnths &lt;- (data$year - data$wido.year)*12 + (data$month - data$wido.month)\n\n\n\nFilter Participants for Analysis\nFilter participants who have experienced widowhood, and for whom at least one observation before and after widowhood is available.\n\n# Rename ID variable and recode into a factor\ndata$id &lt;- factor(data$nomem_encr)\n\n# Select only participants who have experienced widowhood \nwido &lt;- data %&gt;%\n  group_by(id) %&gt;%\n  filter(any(wido.ev == 1)) %&gt;%\n  filter(!is.na(lifesatisfaction))\n\n# Select participants with at least one observation before and one after widowhood\nwido &lt;- wido %&gt;%\n  group_by(id) %&gt;%\n  filter(any(mnths &lt; 0) & any(mnths &gt; 0))\n\n\n\nVariable for Model Performance Indices\nCreate a variable for the model performance indices: the mean of life satisfaction per month, to compare the fixed effects trajectory as estimated by the models, with the observed mean life satisfaction trajectory.\n\n# Create a variable for the model performance indices: the mean of life satisfaction per month\nwido &lt;- wido %&gt;%\n  group_by(mnths) %&gt;%\n  mutate(m_lifesat_per_mnth = mean(lifesatisfaction, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n\n\nSave Data\nSelect relevant variables, and save the data.\n\n# Select only the variables we need\nwido &lt;- wido %&gt;% dplyr::select(c(id, lifesatisfaction, mnths, m_lifesat_per_mnth))\n\n# Save data\nsave(wido, file = \"data/wido.rdata\")\n\n\n\nCreate Objects for Visualisation and Cross-Validation\nCreate objects to use for visualisation and cross-validation. First create a folder to save these.\n\ndir.create(\"objects\")\n\nCreate a basic plot to visualise the model predictions, and save it in the folder.\n\n# Create plot\nplot_base &lt;-\n  ggplot() + theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(colour = \"black\")\n  ) + \n  labs(x = \"Months from Widowhood\", y = \"Life Satisfaction\") + \n  scale_y_continuous(breaks = seq(1, 7, by = 1), limits = c(1, 7)) + \n  scale_x_continuous(breaks = seq(-180, 180, by = 30),\n  limits = c(-180, 180)) + \n  theme(legend.position = \"none\") + \n  coord_fixed(ratio = 45)\n\n# Save it\nsaveRDS(plot_base, file = \"objects/plot_base.rds\")\n\nRandomly distribute the participants in 5 groups, and create training and test data sets for cross-validation, by sampling from these groups. Save the lists of training and test data sets in the folder.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create a unique list of participants and assign them randomly to 5 groups\ngroup &lt;- wido %&gt;% \n  distinct(id) %&gt;% \n  mutate(group = sample(1:5, size = n(), replace = TRUE)) %&gt;% \n  ungroup()\n\n# Join this group information back to the original data\nwido &lt;- wido %&gt;% left_join(group, by = \"id\")\n\n# Create training and test data sets\ntraining1 &lt;- wido %&gt;% filter(group != 1) \ntest1 &lt;- wido %&gt;% filter(group == 1)\n\ntraining2 &lt;- wido %&gt;% filter(group != 2) \ntest2 &lt;- wido %&gt;% filter(group == 2)\n\ntraining3 &lt;- wido %&gt;% filter(group != 3) \ntest3 &lt;- wido %&gt;% filter(group == 3)\n\ntraining4 &lt;- wido %&gt;% filter(group != 4) \ntest4 &lt;- wido %&gt;% filter(group == 4)\n\ntraining5 &lt;- wido %&gt;% filter(group != 5) \ntest5 &lt;- wido %&gt;% filter(group == 5)\n\n# Create lists of these data sets\ntraining_datasets &lt;- list(training1, training2, training3, training4, training5)\ntest_datasets &lt;- list(test1, test2, test3, test4, test5)\n\n# Save these lists\nsaveRDS(training_datasets, file = \"objects/training_datasets.rds\") \nsaveRDS(test_datasets, file = \"objects/test_datasets.rds\") \n\nDone!",
    "crumbs": [
      "Data Preparation"
    ]
  }
]